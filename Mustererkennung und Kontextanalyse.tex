 \documentclass{article} %A4
\usepackage[a4paper,left=1.9cm, right=2.1cm,top = 1.2cm,bottom=2.3cm]{geometry}
\usepackage[utf8]{inputenc}%Umlaute
\usepackage[ngerman]{babel} %Texttrennung
\usepackage{graphicx}	%Grafiken
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{url}
\usepackage{listings}
 \usepackage{color}
\usepackage{hyperref}
\usepackage{framed}
\usepackage{algpseudocode}
\usepackage{tikz}
\usepackage{ upgreek }

\usepackage[labelformat=empty]{caption}
\title{Zusammenfassung - Mustererkennung und Kontextanalyse}
\author{
	Andreas Ruscheinski,
	Marc Meier
}


\begin{document}
\maketitle
\begin{framed}Korrektheit und Vollständigkeit der Informationen sind nicht gewährleistet.
Macht euch eigene Notizen oder ergänzt/korrigiert meine Ausführungen!
\end{framed}

\tableofcontents

\section{Überblick Klassifikation}
	\subsection{Einführendes Beispiel}
	\begin{itemize}
		\item Ziel: Bestimmung von Fischen auf der Basis von Kamerainformationen
		\item Verwendung der Kamera zum Merkmale (Features) bestimmen des aktuellen Fisches: Länge, Helligkeit und Breite
		\item Annahme: Die Modelle (Beschreibung) der Fische unterscheiden sich
		\item Klassifikation: Finde zu gegebenen Merkmalen das am besten passende Modell (Welche Beschreibung der Fische passt am besten zu dem aktuellen Fisch)
	\end{itemize}
	
	\subsection{Entscheidungsgrenzen}
		Komplexe Entscheidungsgrenze $\Rightarrow$ Mehr Parameter werden für die Bestimmung benötigt $\Rightarrow$ Weniger Trainingsdaten stehen zur Bestimmung des einzelnen Parameters zur Verfügung $\Rightarrow$ Parameter wird ungenauer bestimmt und ist anfälliger gegen Schwankungen der Trainingsdaten $\Rightarrow$.
		Mehr Features $\Rightarrow$ höhere Dimensionalität des Merkmalsraums $\Rightarrow$ größere inhärente Komplexität der gegebenen Form von Entscheidungsgrenzen $\Rightarrow$ schlechtere Bestimmung der Parameter
	\subsection{Mustererkennungssysteme}
		\begin{description}
			\item[1) Sensing:] Erfassung der Umwelt mittels Sensoren, z.B: Kamera, Bewegungssensoren, Mikrophon, RFID-Lesegerät, Problem: Eigenschaften und Begrenzungen (Bandbreite, Auflösung, Empfindlichkeit, Verzerrung, Rauschen, Latenz, \dots) des Sensors beeinflussen Problemschwierigkeit 			
			\item[2) Segmentation:] Identifikation der einzelnen Musterinstanzen (Identifikation der für unser Problem relevanten Daten), z.B: Fisch auf dem Fließband
			\item[3) Merkmalsberechnung:] Bestimmung von Features, gesucht sind dabei Features, welche eine Diskrimierungsfähigkeit haben (Zwischen zwei gleichen Klassen ähnlicher Wert, zwischen zwei verschiedenen Klassen großer Werteunterschied) und Invariant gegenüber Signaltransformationen (Rotation, Translation, Skalierung, perspektivische Verzerrung) sind; Problem: Wie kann ich aus einer großen Auswahl an Merkmalen die am besten geeigneten finden?
			\item[4) Klassifikation:] Annahme: Modelle (Grundlegende Eigenschaften) der Klassen (verschiedene Fische) unterscheiden sich; Ziel: Zuweisung von Probleminstanzen aufgrund deren Merkmale zu der am besten entsprechenden Klasse			
			\item[5) Nachbereitung:] Entscheidung auf Basis Klassifikation, Problem: Wie können wir Kontextinformationen nutzen? Können wir verschiedene Klassifikatoren zusammen nutzen? 
		\end{description}
	
	\subsection{Entwurf von Mustererkennungssystemem}
	\begin{description}
		\item[1) Daten sammlen:] Wie kann man wissen, wann eine Menge von Daten ausreichend groß und repräsentativ ist, um das Klassifikationssystem zu trainieren und zu testen? 
		\item[2) Merkmale bestimmen:] Gesucht: Einfach zu extrahierende Merkmale mit hoher Diskriminativität, Invariant gegenüber irrelevanten Transformationen, unempfindlich gegenüber Rauschen, Problem: Wie kann ich A-priori-Wissen nutzen?
		\item[3) Modell auswählen:] Wie erkennt man, wann ein Modell sich in der Klassifikation signifikant von einem anderen Modell – oder vom wahren Modell – unterscheidet? Wie erkennt man, dass man eine Klasse von Modellen zugunsten eines anderen Ansatzes ablehnen sollte? Versuch-und-Irrtum oder gibt es systematische Methoden?
		\item[4) Klassifikator trainieren:] Verwende gesammelte Daten, um Parameter des Klassifikators zu bestimmen
		\item[5) Klassifikator evaluieren:] Wie bewertet man die Leistung? Wie verhindert man Overfitting/Underfitting?
	\end{description}
	Für weitere Informationen sind folgende Referenzen zu konsultieren: \cite[S. 3-16]{dudaPattern}
\section{Grundlagen Signalverarbeitung}
	In diesem Abschnitt werden die Grundlagen der digitalen Signalverarbeitung beschrieben. In der digitalen Signalverarbeitung werden Methoden und Techniken behandelt welche aus  anlogen Sensorwerten eine digitale Information erstellen.
	\subsection{Digitalisierung}
		Die gemessenen Werte der Sensoren werden durch unterschiedliche Ausgangsspannungen realisiert d.h. in Abhängigkeit von der gemessenen Größe ändert sich die gemessene Spannung am Ausgang des Sensors.\\
		Diese analogen Signale werden im ersten Schritt zeitlich diskretisiert d.h. die kontinuierlichen Signale werden durch Abtastung angenähert. Unter Abtastung versteht man die Erhebung eines Wertes zu einem Zeitpunkt. Die Häufigkeit der Abtastung wird in Herz (Hz) angegeben d.h. Abtastung mit 10 Hz entspricht 10 maliges abtasten des analogen Signales innerhalb von einer Sekunde. Durch diesen Schritt erhalten wir eine Folge von gemessenen Spannungen.\\
		Im nächsten Schritt werden die abgetasteten Werte diskretisiert (Diskretisierung der Amplituden) d.h. jeder Spannung wird ein digitaler Wert zugewiesen. Dies geschieht mittels einem A/D-Wandler, welcher entsprechend von Grenzwerten (1/2 Spannung, 1/4 Spannung, 1/8 Spannung) entsprechende Bits setzt und diese Information ausgibt.
		\subsubsection{Dithering}
		Ein Problem bei der Diskretisierung der Amplitude ergibt sich dadurch, dass bei einem sehr geringen Sensorwert das LSB nicht gesetzt wird. Dies hat zur Folge das Informationen verloren gehen. Um dies zu verhindern wird Zufallsrauschen auf den aktuellen Sensorwert addiert. Dadurch wird der Grenzwert manchmal überschritten. So nährt sich der Erwartungswert den Realwert an.
		\subsubsection{Abtastung}

			Ein Signal nur dann kann korrekt abgetastet werden, wenn es keine
			Frequenzanteile enthält, die oberhalb der halben Abtastrate liegen. (Abtasttheorem)\\
			$f_{max} \leq \frac{1}{2}f_{sample}$\\
		Aus dem Abtasttheorem folgt: Wenn wir ein Signal mit $f_{max}$ korrekt abzutasten wollen, müssen wir dieses Signal mit einer Frequenz von $2*f_{max}$ abtasten. 
	\subsection{Lineare Systeme}
		Ein lineares System erfüllt folgende Eigenschaften:
		\begin{description}
			\item[Homogenität] $f(c*a) = c*f(a)$ d.h eine Veränderung des Input-Signales hat eine identische Änderung des Output-Signals zu folge
			\item[Additivität] $f(a+b) = f(a)+f(b)$ d.h wenn das Input-Signal aus zwei überlagerten Signalen besteht können wir diese getrennt Auswerten und anschließend die Ergebnisse addieren
			\item[Translationsinvarianz] $f(n) = y(n) \rightarrow f(n+s) = y(n+s)$ d.h. ein zeitlicher Versatz des Input-Signals hat den selben zeitlichen Versatz im Output-Signal zur Folge
			\item[Kommutativität] $f(a) = b, g(b) = c \rightarrow g(a)=b, f(b) = c$ d.h. wenn mehrere lineare Systeme in einer Reihe verknüpft sind, können diese vertauscht werden ohne das Ergebnis zu beeinflussen
		\end{description}
		Aus diesen Eigenschaften folgt: Ein lineares System ist vollständig durch seine Impulsantwort charakterisiert. Eine Impulsantwort erhalten wir durch Eingabe eines Signals, welches genau an einer Stelle einen Wert größer als 0 hat (Deltafunktion). Die daraus resultierende Antwort beinhaltet alle Eigenschaften des linearen Systemes d.h. unter Verwendung der o.g. Eigenschaften können wir nachfolgend auf Basis der Impulsantwort ermitteln, welches Ergebnis aus anderen Input-Signalen resultiert.
		\subsubsection{Überlagerung}

		Aus den Eigenschaften des linearen Systems folgt: $f(x) = f(x_1+x_2+x_3) = f(x_1)+f(x_2)+f(x_3)$ d.h. wir können das Eingangssignal zerlegen (Decomposition) und die zerlegten Signale wieder zusammenführen (Synthese), ohne dass das Ergebnis der Analyse beeinflusst wird.\\
		Diese Überlegung können wir nutzen um das Eingangssignal in mehrere Deltafunktionen zu zerlegen. Anschließend werden diese analysiert und die Teilergebnisse zusammengefasst. Auf diese Weise wird das Ergebnis aus dem Eingangssignal zu ermitteln.\\
		Des Weiteren ist auch eine Zerlegung das Signal in mehrere Cosinus- und Sinus-Signale interessant(siehe \label{sec-Fourier}).
		\subsubsection{Faltung}
		Um die Faltung zu berechnen benötigen wir ein Eingangssignal und die Impulsantwort des Systemes.\\
		Die Grundidee besteht darin, dass wir das Eingangssignal in einzelne Delta-Impulse zerlegen. Für jeden dieser Delta-Impulse wird die entsprechende verschobene und skalierte Kopie der Impulsantwort berechnet. Anschließend werden alle Impulsantworten addiert.\\
		Hierfür ergibt sich somit folgende Formel: $y[i] = \sum_{j=1}^{M}h[j]*x[i-j]$ mit $h$ ist Impulsantwort und $x$ das Eingangssignal.\\
		Durch eine geeignete Wahl der Impulsantwort können Filter, Ableitungen und Integrale realisiert werden. Im nächsten Abschnitt wird ein Verfahren beschrieben, welches die Faltung nutzt um eine Korrelation zu berechnen.
		\subsubsection{Korrelation}
		Das Ziel in der Korrelation ist die Erkennung eines bekannten Signales $t$ innerhalb eines verrauchten Signales $x$.\\
		Für die Berechnung der Korrelation nutzen wir die Faltung mit der gespiegelten Impulsantwort $y[n] =x[n]*t[-n]$. Als Ergebnis dieser Faltung erhalten wir ein Ausgangssignal $y$, welches signifikate Ausschläge im übereinstimmenden Bereich hat.
		\subsection{Fourier-Transformation}
		\subsubsection{Allgemein}
		Mittels einer Fourier-Transformation können wir unser Eingangssignal in eine Summe von Sinus- und Kosinus-Funktionen zerlegen.\\
		Hierfür müssen folgende Bedingungen gelten (Dirichlet-Bedingungen):
		\begin{enumerate}
			\item Anzahl der Unstetigkeiten innerhalb einer Periode ist endlich
			\item Anzahl der Maxima und Minima innerhalb einer Periode ist endlich
			\item Funktion ist in jeder Periode integrierbar (d.h. die Fläche unter dem Betrag der Funktion ist in jeder Periode endlich)
		\end{enumerate}
		Die Sinus- und Kosinus-Funktionen werden auch Basisfunktionen genannt und bilden einen Vektorraum.
		\subsubsection{Fourier-Transformation Grundideen}
		Nachfolgend werden die Grundideen der Fourier-Transformationen erläutert.\\
		Die Ausgangsidee ist dass jedes Signal durch eine Summe von phasenverschobenen Kosinus-Funktionen beschreiben werden kann. Wir sprechen von einer Phasenverschiebung falls zwei Kosinus-Funktionen unterschiedliche Nullstellen haben (d.h. eine Verschiebung auf der x-Achse). Hierfür ergibt sich folgende Formel: $s[i] = \sum_{k=0}^{N/2} M_{k}*cos(2*\pi*k*i/N+\phi_{k})$ wobei $i = 0,\dots,N-1$, $M_{k}$.\\
		Die zweite Idee ist dass jede phasenverschobene Kosinus-Funktion $M*cos(x+\phi)$ kann durch eine Summe von Kosinus- und Sinus-Funktion ohne Phasenverschiebung repräsentiert werden: $M*cos(x+\phi) = A*cos(x) + B*sin(x)$ mit $A=M*cos(\phi)$ und $B=M*sin(\phi)$ wobei $M$ ist die Amplitude und $\phi$ die Phasenverschiebung. Dies erhalten wir durch den Übergang von Kartesischen-Koordinaten in Polar-Koordinaten in der komplexen Zahlenebene. Dadurch bestehen die  Polarkoordinaten  aus einen Imaginär und einen Realteil.\\
		Da wir uns im diskreten Bereich befinden gilt folgende Eigenschaft: Das Signal aus N Werten ist ein N-dimensionaler Vektor. Wie vorher beschrieben bilden die gesuchten Basisfunktionen einen Vektorraum. Für einen N-dimensionalen Vektorraum benötigen wir also eine Basis mit N orthogonalen Vektoren (d.h. das Skalarprodukt zweier Basis-Vektoren muss 0 sein).\\
		Die diskreten Sinus und Kosinus-Funktionen $c_{k}[i] = sin(2*\pi*k*i/N)$ bzw. $c_{k}[i] = cos(2*\pi*k*i/N)$ sind alle zueinander orthogonal. Da die Summe von 0 bis $N/2$ läuft erhalten wir genau $N/2+1$ phasenverschobene Kosinus-Funktionen, welche jeweils in eine Sinus und eine Kosinus Funktion zerlegt wird. Somit erhalten wir $2*(N/2+1) = N+2$ Basisfunktionen. Da $s_{0} = sin(0)$ und $s_{N/2} = sin(2*pi*N/2*i/N) = sin(\pi*i)$ jeweils Nullvektoren sind, können diese Verworfen werden wodurch wir $N$ Basisfunktionen erhalten.\\
		Unter einer diskreten Fourier-Transformation verstehen wir die Transformation des Signalsvektors in ihre Sinus- und Kosinus-Basis.
		\subsubsection{Diskrete Fourier-Transformation}
		Man unterscheidet bei der diskreten Fourier-Transformation zwischen der Zeit-Domäne und der Frequenz-Domäne. Der Übergang von der Zeit-Domäne in die Frequenz-Domäne wird Diskrete-Fourier-Transformation (DFT) genannt. Der rückwärtige Übergang wird Invers-Diskrete-Fourier-Transformation (IDFT) genannt.\\
		Die Zeit-Domäne $x[]$ besteht aus N-Samples, welche von 0 bis N-1 nummeriert sind. Die Frequenz-Domäne beinhaltet die durch die DFT erhaltenen Real- $ReX[]$ und Imaginär-Teile $ImX[]$, welche jeweils aus $N/2+1$ Elementen besteht. Die Real-Teile beschreiben Amplituden der Kosinus-Wellen, wobei die Imaginär-Teile die Amplituden der Sinus-Wellen bechreiben.\\
		Die Basisfunktionen d.h. die Funktionen die ein Signal x zerlegen:
			$$c_{k}[i]=cos(2*\pi*k*i/N)$$
			$$s_{k}[i]=sub(2*\pi*k*i/N)$$
		wobei k die Wellenzahl ist. $c_{k}$ bzw. $s_{k}$ ist das Signal der Kosinus- bzw. Sinusfunktion die mit der Amplitude in der Fourierzerlegung auftritt. Alle Basisfunktionen müssen genau so lang wie das Signal sein. Der Parameter $k$ gibt die Anzahl der Zyklen innerhalb der Signallänge an. $c_{0} = Re[0]$ ist der Gleichstrom-Versatz (DC-Offset). $s_{0} = Im[0]$ und $s_{N/2} = Im{N/2}$ sind überall 0, also irrelevant für die gesuchte Basis.\\
		Bisher wissen wir welche Basisfunktionen in dem Signal enthalten seien können. Jedoch fehlt uns der Anteil der Basisfunktion in dem Ausgangssignal d.h. uns fehlt noch die konkrete Berechnung der Real- bzw. Imaginar-Teile (Amplituden der Sinus- bzw. Kosinus-Funktionen). Bevor wir uns damit befassen ist noch eine Vorüberlegung notwendig.\\
		Da wir wissen wollen wie ähnlich unser Eingangssignal zu unser Basis ist berechnen wir nachfolgend die Korrelation. Die allgemeine Formel ergibt sich wie folgt:
		$$ \sum_{i=0}^{N-1} x[i]*y[i]$$ mit $x$ als Eingangssignal und $y$ unsere Basis.\\
		Ausgehend von dieser Beobachtung und unseren vorher ermittelten Basisfunktionen ergeben sich nachfolgend die Formeln für die Amplituden:
		$$ReX[k] =\sum_{i=0}^{N-1} x[i]*cos(2*\pi*k*i/N) $$ bzw. $$ImX[k] =\sum_{i=0}^{N-1} x[i]*sin(2*\pi*k*i/N) $$ für $k = 0,\dots,N/2$.
		\subsection{TODO}
		
\section{Merkmale}
	\subsection{Statistische Merkmale}
		\subsubsection{Erwartungswert}
		Der Erwartungswert einer diskreten Zufallsvariable $x$ mit den möglichen Werten $x_{i}$ und zugehörigen Wahrscheinlichkeiten $P(x_{i})$ ist: $E[x] = \sum_{i=1}^{n} (x_{i}*P(x_i))$.\\
		Der Erwartungswert einer kontinuierlichen Zufallsvariable $x$ mit Wertebreich $X$ und zugehöriger Wahrscheinlichkeitsdichte $p(x)$ ist: $E[x] = \int_{X} (x*p(x)) dx$.\\
		Der Erwartungswert einer Zufallsvariable wird auch als Mittelwert $\mu$ bezeichnet.\\
		Für gleichverteilte diskrete Zufallsvariablen x mit Werten $x_1,x_2,\dots,x_n$ gilt: $P(x_i) = 1/n$ und somit: $E[x] = \sum_{i=1}^{n} (x_{i}*\frac{1}{n}) = \frac{1}{n}*\sum_{i=1}^{n} x_{i}$.\\
		Für die Erwartungswerte von Funktionen einer Zufallsvariablen, $f(x)$ gilt: $E[f(x)]=\sum_{i=1}^{n} (f(x_i)*P(x_i))$ bzw. $E[f(x)] = \int_{X} (f(x)*p(x) dx$.
		\subsubsection{Momente}
		Der r-te Moment einer Zufallsvariable x ist $E[x^r]$. Der erste Moment mit $r=1: E[x] \mu$ heißt auch Mittelwert.\\
		Der r-te zentrale Moment ist: $\mu_r = E[(x-E[x])^r] = E[(x-\mu)^r]$.\\ 
		Das zweite zentrale Moment $\mu_2 = E[(x-\mu)^2] = \sigma^2$ heißt auch Varianz; $\sqrt{\sigma^2}$ heißt Standardabweichung $\sigma$.\\
		Die Schiefe einer Verteilung ist ein Maß für ihre Asymmetrie: $skew(x) = \frac{\mu_3}{\sigma^3} = \frac{E[(x-\mu)^3]}{\sigma^3}$. Wenn $skew(x) > 0$ linkssteil (rechtsschief) bzw. $skew(x) < 0$ rechtssteil (linksschief).\\
		Die Wölbung einer Verteilung ist ein Maß für ihrere Spitzheit: $kurt(x) = \frac{\mu_4}{\sigma^4}$. Wenn $kurt(x) < 3$ flach bzw. $kurt(x) > 3$ spitz. Falls x normalverteilt gitl: $kurt(x)=3$.
		\subsubsection{Emprische Werte}
		Die vorliegenden Messwerte $x_1,\dots,x_n$ stellen eine Stichprobe aus der Zufallsvariablen x zugrunde liegenden Verteilung dar. Überlichweise sind die wahren Parameter ($\mu,\sigma$) dieser Verteilung nicht bekannt. Mann muss daher diese Parameter auf Basis der Stichprobe schätzen.\\
		Der empirische Mittelwert: $\overline{x} = \frac{1}{n}*\sum_{i=1}^{n} x_i$ wobei $E[\overline{x}] = \mu$.\\
		Die empirische Varianz ist: $s^2 = \frac{1}{n-1}*\sum_{i=1}^{n}(x_i-\overline{x})^2$ wobei $E[s^2] = \sigma^2$.
		\subsubsection{Lageparameter}
		Lageparameter treffen allgemeine Aussagen über die Position der Verteilung und stellen in gewisser Weise die Lage ihres Schwerpunkts dar. Verschiedene Lagemaße sind dabei unterschiedlich robust, zeigen sich also mehr oder weniger empfindlich gegenüber Ausreißerwerten. Beispiele: Mittelwert, Median, Modus.\\
		Der Median ist der kleinste Wert $x_m$, bei dem die kumultative Verteiltungsfunktion $F_p(x) = \int_{-\inf}^{x} p(z) dz$ einen Wert von $\geq 0.5$ liefert d.h. er teilt die Verteilungsfunktion in zwei (flächenmäßig) gleich große Teile.\\
		Für eine geordnete Stichprobe $x_1\leq\dots\leq x_n$ ist der Median der Wert $x_{(n+1)/2}$ (falls n ungerade) bzw. der Wert $1/2*(x_{n/2}+x_{n/2+1})$. Daraus folgt dass der Median als Lagemaß wesentlich unempfindlicher ist gegenüber Ausreißer als der Mittelwert.\\
		Der Modus einer Verteilung ist der Wert mit der größten Wahrscheinlichkeit. Gibt es nur einen Modus nur einen Modalwert nennt man die Verteilung unimodal, sonst besitzt Verteilung  mehre  Modalwerte und nennt sie desshhalb bimodal.\\
		Es gilt:
		\begin{itemize}
			\item links-steile, rechts-schiefe Verteilung: Modus $<$ Median $<$ Mittelwert
			\item rechts-steile, links-schiefe Verteilung: Modus $>$ Median $>$ Mittelwert
		\end{itemize}
		\subsubsection{Streuungsmaße}
		Streuungsmaße beschreiben die Breite bzw. die Ausdehnung einer Verteilung und somit die Abweichung vom Schwerpunkt. Sie sind somit ein Maß für die Variabilität der Daten. Beispiel: Varianz, Quatile, Interquartilsabstand.
		\begin{itemize}
			\item Quartile: drei Quartile teilen die geordnete Datenmenge in vier gleich große Segmente, wobei das zweite Quartil gleichzeitig den Median darstellt
			\item Interquartilabstand bezeichnet den Abstand zwischen den 1. und den 3. Quartil und umfasst also die Hälfte der Daten und ist analog zum Median, robuster gegen Ausreißer als die Varianz-
		\end{itemize}	
		\subsubsection{Standardisierung}
		\begin{itemize}
			\item Problem: Normalverteilung $N(\mu,\sigma^2)$ hat nicht zwangsläufig eine Fläche von 1 unter der Kurve
			\item Lösung: Transformation mit $z = \frac{x-\mu}{sigm}$ dadurch Normalverteilung $N(0,1)$ als Ergebnis
			\item Mahalanobis-Abstand: $r=\frac{|x-\mu|}{\sigma}$ (Z-Score) (Musst die Distanz zwischen x und $\mu$ in Einheiten der Standardabweichung)
			\item wenn $z=\frac{x-\mu}{\sigma}$ dann gilt:
			\begin{itemize}
				\item $skew(x) = \frac{E[(x-\mu)^3]}{\sigma^3} = E[(\frac{x-\mu}{\sigma})^3]  = E[z^3]$
				\item $kurt(x) = \frac{E[(x-\mu)^4]}{\sigma^4} = E[(\frac{x-\mu}{\sigma})^4]  = E[z^4]$
				\item d.h. Schiefe und Wölbung von x sind das 3 bzw. 4 Moment der standardisierten Verteilung x
			\end{itemize}
			\item für multivarianten Fall mit $\Upsigma$ die Kozarianzmatrix gilt: $z=\Upsigma^{-1/2}*(x-\mu)$ d.h. aus $N(\mu,\Upsigma)$ wird $N(0,I)$
		\end{itemize}
		\subsubsection{Korrelation}
		\begin{itemize}
			\item x,y Zufallsvariablen und $\mu_x,\mu_y$ die Erwartungswerte und $\sigma_x,\sigma_y$ die Standardabweichung
			\item Korreleationkoeffizient $p_{xy}$ ist ein Maß für die lineare Abhängigkeit
			\item $p_{xy} = \frac{E[(x-\mu_x)*(y-\mu-y)]}{\sigma_x*\sigma_y}$
			\item wenn $|p_{xy}| = 1$ denn lineare Abhängigkeit zwischen x und y
			\item wenn x und y unabhängig $\rightarrow p_{xy} = 0$ anders rum nicht
			\item emprische Korreleation $r_{xy}$ zweier Stichproben $x=x_i,y=y_i$ misst die Abhängigkeit zweier Stichproben: $r_{xy} = \frac{x'*y'}{||x'||*||y'||}$ mit $x'_i = x_i-\overline{x},y'_i=y_i-\overline{y}$
		\end{itemize}
		\subsubsection{Einsatz statistischer Merkmale}
		\begin{itemize}
			\item problemunabhängig d.h. können immer eingesetzt werden da kein Vorwissen über die Problemstruktur 
		\end{itemize}
	\subsection{Merkmalstypen}
		\begin{center}
			\begin{tabular}{|c|c|c|c|}
				\hline Niveau & Operationen & Lageparameter & Beispiel \\ 
				\hline nominal & $\{=,\neq\}$ & Modus &  Geschlecht\\ 
				\hline ordinal & $+\{>,<\}$ & +Median  & Bundesligatabelle \\ 
				\hline intervall & $+\{-,+\}$ & +Erwartungswert &  Geburtsjahr\\ 
				\hline ratio & $+\{/,*\}$ & +geom. Mittel & Wohnfläche \\ 
				\hline 
			\end{tabular} 
		\end{center}
		\begin{itemize}
			\item Intervall und Ratio Saklen sind metrisch d.h. Abstandsbegriff ist sinnvoll
			\item Addition auf Intervallsaklen für Abstände sinnvoll
			\item Ratioskalen haben Nullpunkt
			\item Nominal- und Ordinalskalen sind immer diskret
		\end{itemize}
	\subsection{Merkmale für Zeitreihen}
		\subsubsection{Summarische Merkmale}
		\begin{itemize}
			\item Gegeben ein Signal $x = x_1,\dots,x_n$
			\item Zero Crossing Rate (im Ortsbereich): $zcr(x) = \frac{1}{n-1} \sum_{i=2}^{n} \mathfrak{I}\{x_i*x_{i-1} < 0\}$ mit $\mathfrak{I}\{A\} = 1$ falls A wahr
			\item Energie (im Orts- und im Frequenzbereich): $en(x) = \sum_{i=1}^{n} x_i^2$
			\item Entropie (im Orts- und im Frequenzbereich); $ent(x) = -\sum_{i=1}^{n} x_i^**log(x_i^*)$ wobei $x_i^* = \frac{x_i}{\sum_{j=1}^{n}x_j}$
			\item Schwerpunkt (spectral centroid)(Frequenzbereich): $sc(x) = (\sum_{k=1}^{K}f_k*|x_k|^2)/(\sum_{k=1}^{k}|x_k|^2)$
			\item Bandbreite (bandwidth)(Frequenzbereich) $bw(x) = (\sum_{k=1}^{k}(f_k-sc)^2*|x_k|^2)/(\sum_{k=1}^{k}|x_k|^2)$
		\end{itemize}
		\subsubsection{Autokorrelation, Grundfrequenz}
		\begin{itemize}
			\item Signal $x = x_1,\dots,x_n$
			\item Autokorrelation: $R_x(\tau) = (x\star x)(\tau) = \sum_{i}^{n} x_i*x_{i-\tau}$
			\item $\tau$ = Verzögerungsparameter (Lag)
			\item üblich Autokorrelation: $ACF_x(\tau) = \frac{R_x(\tau)}{R_x(0)}$
			\item Verwendung: Bestimmung der Grundfrequenz $->$ das zweite Maximum der ACF liefert die $1/f_0$ Periodendauer
			\item d.h. Tau als Parameter $\rightarrow$ durchprobieren bis zweite Maximum gefunden, Begründung: $\tau = 0$ ist immer das erste Maxima
			\item Maximum des Fourierspektrums nicht geeignet da im niedriegen Frequenzbereich nur eine grobe Auflösung
		\end{itemize}
		\subsubsection{Phasendifferenz}
		\begin{itemize}
			\item zwei Sinus Signale: $x_i = sin(w*i), y_i = sin(w*i+\phi)$ d.h. gleiche Frequenz aber Phasenverschiebung
			\item $\phi = arccos(r_{xy})$
		\end{itemize}
	\subsection{Merkmale für kinetische Systeme ????}
\section{Bayessche Entscheidungstheorie}
	\subsection{Einfürhung}	
	\begin{itemize}
		\item Natur hat einen Zustand $\omega$ aus einer Wertemenge $\Omega = \{\omega_1,\omega_2,\dots\}$
		\item Beispiel $\omega$: Fisch ist Wolfsbarsch ($\omega_1$) oder Lachs($\omega_2$) 
		\item $\omega$ ist meistens unbekannt d.h. eine Zufallsvariable
		\item Wenn es gleich viele Lachse und Wolfsbarsche gibt, ist es gleichwahrscheinlich die eine oder andere Art zu tippen
		\item a priori Wahrscheinlichkeit = Vorwissen
		\item P("Barsch") + P("Lachs) = 1
		\item wenn wir nur die a priori Wahrscheinlichkeitne kennen ist die Wahl mit der größten Wahrscheinlichkeit immer die beste
		\item Typischerweise haben wir mehr Informationen d.h. zusätzlichen Wissen für unsere Entscheidung
		\item Beispiel: die Helligkeit (Merkmal $x \in \mathbb{R}$) hängt von der Fischart $\omega$ ab d.h. wir haben klassenabhängige Wahrscheinlichkeiten
		\item $p(x|\omega)$ ist die klassenabhängige Wahrscheinlichkeit des Merkmals x 
		\item uns interessiert $P(\omega_i|x)$ also die Wahrscheinlichkeit dass die Welt den Zustand $\omega_i$ hat nachdem wir x beobachtet haben
		\item Verwendung: Satz von Bayes: $P(\omega_i|x) = \frac{p(x|\omega_i)*P(\omega_i)}{p(x)}$ mit $p(x) = \sum_{i=1}^{2} p(x|\omega_i) P(\omega_i)$ (für unseren Fall von zwei Fischen)
		\begin{itemize}
			\item $P(\omega_i|x)$: Die Wahrscheinlichkeit des wahren Zustands, nachdem wir den Beweis haben (Posteriori)
			\item $p(x|\omega_i$: Die Passfähigkeit des Beweises x zum möglichen wahren Zustand $\omega$ (Likelihood)
			\item $P(\omega_i$): Die Wahrscheinlichkeit des wahren Zustands bevor wir den Beweis gesehen haben
		\end{itemize}
		\item Nun Entscheidung für $\omega$ mit der größten Posteriori $P(\omega_i|x)$ 
	\end{itemize}
	\subsection{Bayes’sche Entscheidungstheorie}
		\subsubsection{Definitionen}
		\begin{itemize}
			\item Merkmalsvektor x als Element eines d-dimensionalen Euklidischen Merkmalsraums $R^d$ d.h. $x \in R^d$
			\item $\Omega = \{\omega_i,\omega_2,\dots,\omega_c\}$ eine endliche Menge von möglichen Zuständen (Klassen, Kategorien) 
			\item $A = \{\alpha_1,\dots,\alpha_a\}$ eine endliche Menge von möglichen Aktionen
			\item Eine Kostenfunktion $\Lambda(\alpha_i|\omega_j)$ die angiebt welche Kosten die Aktion $\alpha_i$ verursacht wenn der Zustand $\omega_j$ ist
		\end{itemize}
		\subsubsection{Bayes’sche Entscheidungstheorie - formal}
		\begin{itemize}
			\item $p(x|\omega_j)$ und $P(\omega_j)$ sind wie vorher die klassenbedingte Wahrscheinlichkeit von x gegeben $\omega_j$ bzw. die a-priori Wahrscheinlichkeit von $\omega_j$
			\item Es gilt analog für die a-posteriori-Wahrscheinlicht: $p(\omega_j|x) = \frac{p(x|\omega_j)*P(\omega_j)}{p(x)}$ wobei $p(x) = \sum_{i=1}^{c} p(x|\omega_i)P(\omega_i)$
			\item gegeben ein Merkmalsvektor x. Nun soll eine Aktion $\alpha_i$ durchzuführen. Welche kosten entstehen dabei?
			\item Wenn Zustand $\omega_j$ ist sind die Kosten von $\alpha_i$ nach Def: $\Lambda(\alpha_i|\omega_j)$
			\item Zustand $\omega_j$ ist jedoch nicht gegeben, wir kennen nur $P(\omega_j|x)$. Können aber den Erwartungswert der Kosten d.h. das bedingte Risiko $R(\alpha_i|x)$ bestimmen: $R(\alpha_i|x) = E_{\omega|x}[\Lambda(\alpha_i|\omega)] = \sum_{j=1}^{c} \Lambda(\alpha_i|\omega_j)P(\omega_j|x)$
			\item Bayessche Entscheidungsregel besagt nun: wenn x gegeben, wähle Aktion $\alpha_i$ für die das bedingte Risiko $R(\alpha_i|x)$ minimal ist
			\item sei $\alpha(x)$ eine Entscheidungsregel welche für ein gegebenes x eine Aktion $\alpha_i$ auswählt
			\item dann ist das Gesamtrisiko unter dieser Entscheidungsregel: $R_\alpha = \int R(\alpha(x)|x)p(x) dx$
			\item $R_\alpha$ wird minimal wenn man für jeden Punkt x das Minimum wählt d.h. die Aktion $\alpha_i$ für die $R(\alpha_i|x)$ minimal ist
			\item das minimale unvermeidbare Risiko $R^*$ heißt Bayes-Risiko
		\end{itemize}
		\subsubsection{Anwendung}
		\begin{itemize}
			\item ???
		\end{itemize}
		\subsubsection{Klassifikation und Diskriminanten}
		\begin{itemize}
			\item Klassifikatoren können durch Diskriminantenfunktionen repräsentiert erden
			\item Klassifikator für die Klassen $\omega_1,\dots,\omega_c$ besteht aus einer Menge von Diskriminantenfunktionen $g_i(x),i\in\{1,\dots,c\}$
			\item dieser Klassfikator weisst x einer Klasse $\omega_i$ gdw. $g_i(x) > g_j(x), \forall j\neq i$
			\item Bayes-Klassifikator für den allgemeinen Fall mit Risiko kann wie folgt definiert werden: $g_i(x) = -R(\alpha_i|x)$
			\item für die Klassifikation mit minimalen Fehler vereinfacht sich dies zu: $g_i(x) = P(\omega_i|x) \propto p(x|\omega_i)P(\omega_i)$
			\item für eine gegebene Menge von Diskriminanten $g_i(x)$ liefert eine Transformation mit einer monoton wachsenden Funktion $f()$ eine äquivalente Menge von Diskriminanten $f(g_i(x))$
			\item dadurch alternative Formulierungen:
			\begin{itemize}
				\item $g_i(x) = P(\omega_i|x) = \frac{p(x|\omega_i)*P(\omega_i)}{p(x)}$
				\item $g_i(x) =p(x|\omega_i)*P(\omega_i)$
				\item $g_i(x) = ln(p(x|\omega_i))*ln(P(\omega_i))$
			\end{itemize}
			\item Ein Klassifikator (Entscheidungsregel) für $c$ Klassen zerlegt den Merkmalsraum $\mathbb{R}^d$ in maximal c Entscheidungsregionen $R_i,i\in \{1,\dots,c\}$
			\item Falls für einen Merkmalsvektor $x$ gilt $g_i(x) > g_j(x),\forall j\neq i$ dann liegt x in der Entscheidungsregion $R_i$ und x wird als $\omega_i$ klassifiziert
			\item verschiedene Entscheidungsregionen werden durch Entscheidungsgrenzen voneinander getrennt
			\item Entscheidungsgrenzne sind die Bereiche in denen die beiden größten Diskriminanten denselben Wert annehmen
		\end{itemize}
		\subsubsection{Normalverteilung}
		\begin{itemize}
			\item Formel (univariant): $p(x) = N(x;\mu,\sigma^2) = \frac{1}{\sqrt{2*\pi}\sigma}\exp[-\frac{1}{2}(\frac{x-\mu}{\sigma})^2]$
			\item Mittelwert (erster Moment): $\mu$, Varianz (zweite zentrale Moment): $\sigma^2$
			\item Der zentrale Grenzwertsatz: Die Summe von n unabhängigen Zufallsvariablen kovergiert gegen Normalverteilung mit $n\rightarrow \infty$. Da viele natürliche Vorgänge einer großen Zahl unabhängiger Störfaktoren unterliegen, ist die Normalverteilungsdannahme sinnvoll.
			\item hat hohe Entropie d.h. die Verteilung mit der größten Unsicherheit über die Werte einer Zufallsstichprobe
			\item Formel (multivariant): $N(x;\mu,\Sigma) = \frac{1}{(2*\pi)^{d/2}|\Sigma|^{1/2}}*\exp[-\frac{1}{2}*(x-\mu)^t\Sigma^{-1}(x-\mu)]$ 
			\begin{itemize}
				\item $x = (x_1,\dots,x_d)^t$ ein d-dimensionaler Spaltenvektor
				\item $\mu = (\mu_1,\dots,\mu_d)^t$ ein d-dimensnaler Mittelwert-Vektor
				\item $\Sigma$ ist eine $d\times d$ Kovarianzmatrix, $|\Sigma|$ die Diskriminante, $\Sigma^{-1}$ Inverse Matrix
				\item $\mu = E[x] = \int x*p(x)) dx$ (können Komponentenweise bestimmt werden $\mu_i = E[x_i]$)
				\item $\Sigma = E[(x-\mu)(x-\mu)^t] ) = \int (x-\mu)(x-\mu)^tp(x)dx$ ($\sigma_{ij} = E[(x_i-\mu_i)(x_j-\mu_j)]$)
			\end{itemize}
			\item für Kovarianzmatrix $\Sigma$ gilt:
			\begin{itemize}
				\item $\sigma_{ii}$ sind die Varianzen der entsprechenden $x_i$ also $\sigma^2_i$
				\item $\sigma_{ij}$ sind Kovarianzen von $x_i$ und $x_j$, Wenn $x_i$ und $x_j$ unabhängig dann gilt $\sigma_{ij} = 0$
				\item falls alle nicht-diagonalelemente Null sind, ist die multivariante Verteilung einfach das Produkt der d univarianten Verteilungen der Komponenten von x
				\item die Kovarianzmatrix ist symmetrisch und positiv (semi-) definit (M positiv definit: $x^tMx>0 \forall x \neq0$)
			\end{itemize}
			\item Stichproben aus multivarianter Gaußverteilungen bilden typischerweise eine Häufung deren Mittelpunk von $\mu$ und deren Form von $\Sigma$ definiert wird
			\item Orte gleicher Wahrscheinlichkeitsdichte liegen auf Hyperellipsoiden für die $(x-\mu)^t\Sigma^{-1}(x-\mu)$ konstant ist
			\item die Hauptachsen der Hyperellipsoide sind die Eigenvektoren von $\Sigma$, die Eigenwerte geben die Länge dieser Achsen an
			\item $r^2 = (x-\mu)^t\Sigma^{-1}(x-\mu)$ ist die quadrierte Mahalanobis-Distanz; Konturen konstanter Dichte sind Hpyerellipsoide konstanter Mahalanobis-Distanz zu $\mu$
		\end{itemize}
	\subsection{Diskriminanten für die Normalverteilung}
	\begin{itemize}
		\item vorher gezeigt dass Diskriminante $g_i(x) = ln(p(x|\omega_i))*ln(P(\omega_i))$ Klassifikation mit minimaler Fehlerrate liefert
		\item wenn $p(x|\omega_i) = N(\mu_i,\Sigma_i)$ einsetzen in $g_i(x)$ : $g_i(x) = -\frac{1}{2}(x-\mu_i)\Sigma_i^{-1}(x-\mu_i)-\frac{d}{2}ln(2\pi)-\frac{1}{2} ln(|\Sigma_i|)+ln(P(\omega_i))$ $\rightarrow$ Entfernung von $-\frac{d}{2}ln(2\pi)$ da konstant und nicht von der Klasse abhängt
		\item Also erhalten wir: $g_i(x)$ : $g_i(x) = -\frac{1}{2}(x-\mu_i)^t\Sigma_i^{-1}(x-\mu_i)-\frac{1}{2} ln(|\Sigma_i|)+ln(P(\omega_i))$
		\item Entscheidungsgrenzen:
		\begin{itemize}
			\item $\Sigma_i = \sigma^2I$ d.h. Alle Merkmale unabhängig, gleiche Varianzen, auf der Hauptdiagonalen stehen die gleichen Werte; Die Klassen bilden kugelförmige Häufungen gleicher Größe um ihre jeweiligen Mittelpunkte.
			\begin{itemize}
				\item es gilt: $\Sigma_i^{-1} = (1/\sigma^2)I$ und $|\Sigma_i| = \sigma^{2d}$
				\item einsetzen in Formel: $g_i(x) = -\frac{1}{2\sigma^2}(x-\mu_i)^t(x-\mu_i)-\frac{1}{2} ln(\sigma^{2d})+ln(P(\omega_i))$ 
				\item Konstanten entfernen: $g_i(x) = -\frac{1}{2\sigma^2}(x-\mu_i)^t(x-\mu_i)+ln(P(\omega_i))$ 
				\item Euklidische Distanz: $(x-\mu_i)^t(x-\mu_i) = ||x-\mu_i||^2$
				\item Also:  $g_i(x) = -\frac{||x-\mu_i||^2}{2\sigma^2}+ln(P(\omega_i))$ 
				\item Expandieren:  $g_i(x) = -\frac{1}{2\sigma^2}(x^tx-2\mu_i^tx+\mu_i^t\mu)+ln(P(\omega_i))$ 
				\item Also erhalten wir eine lineare Diskriminante: $g_i(x) = w_i^t*x+ w_{i0}$ (lineare Maschine, $w_{i0}$ ist Schwellwert)
				\item d.h. die Entscheidungsgrenzen zwischen den Regionen werden durch die lineare Gleichung: $g_i(x) = g_j(x)$ definiert (Hyperebenen)
				\item die Hyperebene ist orthogonal zum Vektor w, der Verbindungsline zwischen den beiden Mittelwertvektoren
				\item Line wird in $x_0$ geschnitten falls die a-priori-Wahrscheinlichkeiten gleich sind, liegt $x_0$ zwischen $\mu_i$ und $\mu_j$
			\end{itemize}
			\item $\Sigma_i = \Sigma$ d.h. alle Klassen haben gleiche Kovarianzmatrix d.h.Die Klassen bilden hyperellipsoide Häufungen gleicher Größe, Form und Orientierungen um ihre jeweiligen Mittelpunkte.
			\begin{itemize}
				\item wieder einsetzen in $g_i$: $g_i(x) = -\frac{1}{2}(x-\mu_i)^t\Sigma_i^{-1}(x-\mu_i)-\frac{1}{2} ln(|\Sigma_i|)+ln(P(\omega_i))$ 
				\item Konstanten entfernen:  $g_i(x) = -\frac{1}{2}(x-\mu_i)^t\Sigma_i^{-1}(x-\mu_i)+ln(P(\omega_i))$
				\item Ausmultiplizieren und Symmetrie von $\Sigma$ liefert wieder lineare Maschine 
				\item nicht mehr orthogonal zur Verbindungslinie der Mittelwerte, bei gleicher a-priori-Wahrscheinlicht $x_0$ immer noch auf halben Wege zwischen $\mu_i$ und $\mu_j$
			\end{itemize}
			\item $\Sigma_i = beliebig$
			\begin{itemize}
				\item einsetzen und ausmultiplizieren ergibt Quadratische-Diskriminante
				\item beliebige Normalverteilungen führen zu Entscheidungsgrenzen welche allgemeine Hyperquadriken sind
				\item Bereits kleine Anzahl von Klassen können die Entscheidungsgrenzen komplexe Formen annehmen lassen
			\end{itemize}
		\end{itemize}
	\end{itemize}
	\subsection{Empirischer Fall}
	\begin{itemize}
		\item im Anwendungsfall eher unrealisitisch das man alle Parameter kennt d.h. muss diese auf Basis von Traningsdaten schätzen
		\item wenn man Normalverteilung annehmen möchte:
		\begin{itemize}
			\item $\hat{P}(\omega_i) = \frac{n_i}{n}$
			\item $\hat{\mu}_i = \frac{1}{n_i}\sum_{j=1}^{n_i} x_{ij}$
			\item $\hat{\Sigma} = \frac{1}{n-c}\sum_{i=1}^{c}\sum_{j=1}^{n_i}(x_{ij}-\hat{\mu}_i)(x_{ij}-\hat{\mu}_i)^t$
			\item mit $n_i$ Anzahl der Traningsdatensätze für die Klassei, n Anzahl aller Traningsdatensätze und $x_{ij}$ Traningsdatensatz Nummer j für Klasse i
			\item dies nennt man Lineare Diskriminazanalyse
		\end{itemize}
		\item für Fall 3:
		\begin{itemize}
			\item $\hat{P}(\omega_i) = \frac{n_i}{n}$
			\item $\hat{\mu}_i = \frac{1}{n_i}\sum_{j=1}^{n_i} x_{ij}$
			\item $\hat{\Sigma_i} = \frac{1}{n_i-1}\sum_{j=1}^{n_i}(x_{ij}-\hat{\mu}_i)(x_{ij}-\hat{\mu}_i)^t$
			\item dies nennt man Quadratische Diskriminazanalyse
		\end{itemize}
		\item Bias/Variance Tradeoff = ???
	\end{itemize}
	\subsection{Diskrete Merkmale}
	\begin{itemize}
		\item Integrale durch Summen ersetzen, sonst alles wie bisher
	\end{itemize}
\bibliography{library}
\bibliographystyle{plain}

\end{document}