\documentclass{article} %A4
\usepackage[a4paper,left=1.9cm, right=2.1cm,top = 1.2cm,bottom=2.3cm]{geometry}
\usepackage[utf8]{inputenc}%Umlaute
\usepackage[ngerman]{babel} %Texttrennung
\usepackage{graphicx}	%Grafiken
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{url}
\usepackage{listings}
 \usepackage{color}
\usepackage{hyperref}
\usepackage{framed}
\usepackage{algpseudocode}
\usepackage{tikz}
\usepackage{ upgreek }

\usepackage[labelformat=empty]{caption}
\title{Zusammenfassung - Mustererkennung und Kontextanalyse}
\author{
	Andreas Ruscheinski,
	Marc Meier
}


\begin{document}
\maketitle
\begin{framed}Korrektheit und Vollständigkeit der Informationen sind nicht gewährleistet.
Macht euch eigene Notizen oder ergänzt/korrigiert meine Ausführungen!
\end{framed}

\tableofcontents

\section{Überblick Klassifikation}
	\subsection{Einführendes Beispiel}
	\begin{itemize}
		\item Ziel: Bestimmung von Fischen auf der Basis von Kamerainformationen
		\item Verwendung der Kamera zum Merkmale (Features) bestimmen des aktuellen Fisches: Länge, Helligkeit und Breite
		\item Annahme: Die Modelle (Beschreibung) der Fische unterscheiden sich
		\item Klassifikation: Finde zu gegebenen Merkmalen das am besten passende Modell (Welche Beschreibung der Fische passt am besten zu dem aktuellen Fisch)
	\end{itemize}
	
	\subsection{Entscheidungsgrenzen}
		Komplexe Entscheidungsgrenze $\Rightarrow$ Mehr Parameter werden für die Bestimmung benötigt $\Rightarrow$ Weniger Trainingsdaten stehen zur Bestimmung des einzelnen Parameters zur Verfügung $\Rightarrow$ Parameter wird ungenauer bestimmt und ist anfälliger gegen Schwankungen der Trainingsdaten $\Rightarrow$.
		Mehr Features $\Rightarrow$ höhere Dimensionalität des Merkmalsraums $\Rightarrow$ größere inhärente Komplexität der gegebenen Form von Entscheidungsgrenzen $\Rightarrow$ schlechtere Bestimmung der Parameter
	\subsection{Mustererkennungssysteme}
		\begin{description}
			\item[1) Sensing:] Erfassung der Umwelt mittels Sensoren, z.B: Kamera, Bewegungssensoren, Mikrophon, RFID-Lesegerät, Problem: Eigenschaften und Begrenzungen (Bandbreite, Auflösung, Empfindlichkeit, Verzerrung, Rauschen, Latenz, \dots) des Sensors beeinflussen Problemschwierigkeit 			
			\item[2) Segmentation:] Identifikation der einzelnen Musterinstanzen (Identifikation der für unser Problem relevanten Daten), z.B: Fisch auf dem Fließband
			\item[3) Merkmalsberechnung:] Bestimmung von Features, gesucht sind dabei Features, welche eine Diskrimierungsfähigkeit haben (Zwischen zwei gleichen Klassen ähnlicher Wert, zwischen zwei verschiedenen Klassen großer Werteunterschied) und Invariant gegenüber Signaltransformationen (Rotation, Translation, Skalierung, perspektivische Verzerrung) sind; Problem: Wie kann ich aus einer großen Auswahl an Merkmalen die am besten geeigneten finden?
			\item[4) Klassifikation:] Annahme: Modelle (Grundlegende Eigenschaften) der Klassen (verschiedene Fische) unterscheiden sich; Ziel: Zuweisung von Probleminstanzen aufgrund deren Merkmale zu der am besten entsprechenden Klasse			
			\item[5) Nachbereitung:] Entscheidung auf Basis Klassifikation, Problem: Wie können wir Kontextinformationen nutzen? Können wir verschiedene Klassifikatoren zusammen nutzen? 
		\end{description}
	
	\subsection{Entwurf von Mustererkennungssystemem}
	\begin{description}
		\item[1) Daten sammlen:] Wie kann man wissen, wann eine Menge von Daten ausreichend groß und repräsentativ ist, um das Klassifikationssystem zu trainieren und zu testen? 
		\item[2) Merkmale bestimmen:] Gesucht: Einfach zu extrahierende Merkmale mit hoher Diskriminativität, Invariant gegenüber irrelevanten Transformationen, unempfindlich gegenüber Rauschen, Problem: Wie kann ich A-priori-Wissen nutzen?
		\item[3) Modell auswählen:] Wie erkennt man, wann ein Modell sich in der Klassifikation signifikant von einem anderen Modell – oder vom wahren Modell – unterscheidet? Wie erkennt man, dass man eine Klasse von Modellen zugunsten eines anderen Ansatzes ablehnen sollte? Versuch-und-Irrtum oder gibt es systematische Methoden?
		\item[4) Klassifikator trainieren:] Verwende gesammelte Daten, um Parameter des Klassifikators zu bestimmen
		\item[5) Klassifikator evaluieren:] Wie bewertet man die Leistung? Wie verhindert man Overfitting/Underfitting?
	\end{description}
	Für weitere Informationen sind folgende Referenzen zu konsultieren: \cite[S. 3-16]{dudaPattern}
\section{Grundlagen Signalverarbeitung}
	In diesem Abschnitt werden die Grundlagen der digitalen Signalverarbeitung beschrieben. In der digitalen Signalverarbeitung werden Methoden und Techniken behandelt welche aus  anlogen Sensorwerten eine digitale Information erstellen.
	\subsection{Digitalisierung}
		Die gemessenen Werte der Sensoren werden durch unterschiedliche Ausgangsspannungen realisiert d.h. in Abhängigkeit von der gemessenen Größe ändert sich die gemessene Spannung am Ausgang des Sensors.\\
		Diese analogen Signale werden im ersten Schritt zeitlich diskretisiert d.h. die kontinuierlichen Signale werden durch Abtastung angenähert. Unter Abtastung versteht man die Erhebung eines Wertes zu einem Zeitpunkt. Die Häufigkeit der Abtastung wird in Herz (Hz) angegeben d.h. Abtastung mit 10 Hz entspricht 10 maliges abtasten des analogen Signales innerhalb von einer Sekunde. Durch diesen Schritt erhalten wir eine Folge von gemessenen Spannungen.\\
		Im nächsten Schritt werden die abgetasteten Werte diskretisiert (Diskretisierung der Amplituden) d.h. jeder Spannung wird ein digitaler Wert zugewiesen. Dies geschieht mittels einem A/D-Wandler, welcher entsprechend von Grenzwerten (1/2 Spannung, 1/4 Spannung, 1/8 Spannung) entsprechende Bits setzt und diese Information ausgibt.
		\subsubsection{Dithering}
		Ein Problem bei der Diskretisierung der Amplitude ergibt sich dadurch, dass bei einem sehr geringen Sensorwert das LSB nicht gesetzt wird. Dies hat zur Folge das Informationen verloren gehen. Um dies zu verhindern wird Zufallsrauschen auf den aktuellen Sensorwert addiert. Dadurch wird der Grenzwert manchmal überschritten. So nährt sich der Erwartungswert den Realwert an.
		\subsubsection{Abtastung}

			Ein Signal nur dann kann korrekt abgetastet werden, wenn es keine
			Frequenzanteile enthält, die oberhalb der halben Abtastrate liegen. (Abtasttheorem)\\
			$f_{max} \leq \frac{1}{2}f_{sample}$\\
		Aus dem Abtasttheorem folgt: Wenn wir ein Signal mit $f_{max}$ korrekt abzutasten wollen, müssen wir dieses Signal mit einer Frequenz von $2*f_{max}$ abtasten. 
	\subsection{Lineare Systeme}
		Ein lineares System erfüllt folgende Eigenschaften:
		\begin{description}
			\item[Homogenität] $f(c*a) = c*f(a)$ d.h eine Veränderung des Input-Signales hat eine identische Änderung des Output-Signals zu folge
			\item[Additivität] $f(a+b) = f(a)+f(b)$ d.h wenn das Input-Signal aus zwei überlagerten Signalen besteht können wir diese getrennt Auswerten und anschließend die Ergebnisse addieren
			\item[Translationsinvarianz] $f(n) = y(n) \rightarrow f(n+s) = y(n+s)$ d.h. ein zeitlicher Versatz des Input-Signals hat den selben zeitlichen Versatz im Output-Signal zur Folge
			\item[Kommutativität] $f(a) = b, g(b) = c \rightarrow g(a)=b, f(b) = c$ d.h. wenn mehrere lineare Systeme in einer Reihe verknüpft sind, können diese vertauscht werden ohne das Ergebnis zu beeinflussen
		\end{description}
		Aus diesen Eigenschaften folgt: Ein lineares System ist vollständig durch seine Impulsantwort charakterisiert. Eine Impulsantwort erhalten wir durch Eingabe eines Signals, welches genau an einer Stelle einen Wert größer als 0 hat (Deltafunktion). Die daraus resultierende Antwort beinhaltet alle Eigenschaften des linearen Systemes d.h. unter Verwendung der o.g. Eigenschaften können wir nachfolgend auf Basis der Impulsantwort ermitteln, welches Ergebnis aus anderen Input-Signalen resultiert.
		\subsubsection{Überlagerung}

		Aus den Eigenschaften des linearen Systems folgt: $f(x) = f(x_1+x_2+x_3) = f(x_1)+f(x_2)+f(x_3)$ d.h. wir können das Eingangssignal zerlegen (Decomposition) und die zerlegten Signale wieder zusammenführen (Synthese), ohne dass das Ergebnis der Analyse beeinflusst wird.\\
		Diese Überlegung können wir nutzen um das Eingangssignal in mehrere Deltafunktionen zu zerlegen. Anschließend werden diese analysiert und die Teilergebnisse zusammengefasst. Auf diese Weise wird das Ergebnis aus dem Eingangssignal zu ermitteln.\\
		Des Weiteren ist auch eine Zerlegung das Signal in mehrere Cosinus- und Sinus-Signale interessant(siehe \label{sec-Fourier}).
		\subsubsection{Faltung}
		Um die Faltung zu berechnen benötigen wir ein Eingangssignal und die Impulsantwort des Systemes.\\
		Die Grundidee besteht darin, dass wir das Eingangssignal in einzelne Delta-Impulse zerlegen. Für jeden dieser Delta-Impulse wird die entsprechende verschobene und skalierte Kopie der Impulsantwort berechnet. Anschließend werden alle Impulsantworten addiert.\\
		Hierfür ergibt sich somit folgende Formel: $y[i] = \sum_{j=1}^{M}h[j]*x[i-j]$ mit $h$ ist Impulsantwort und $x$ das Eingangssignal.\\
		Durch eine geeignete Wahl der Impulsantwort können Filter, Ableitungen und Integrale realisiert werden. Im nächsten Abschnitt wird ein Verfahren beschrieben, welches die Faltung nutzt um eine Korrelation zu berechnen.
		\subsubsection{Korrelation}
		Das Ziel in der Korrelation ist die Erkennung eines bekannten Signales $t$ innerhalb eines verrauchten Signales $x$.\\
		Für die Berechnung der Korrelation nutzen wir die Faltung mit der gespiegelten Impulsantwort $y[n] =x[n]*t[-n]$. Als Ergebnis dieser Faltung erhalten wir ein Ausgangssignal $y$, welches signifikate Ausschläge im übereinstimmenden Bereich hat.
		\subsection{Fourier-Transformation}
		\subsubsection{Allgemein}
		Mittels einer Fourier-Transformation können wir unser Eingangssignal in eine Summe von Sinus- und Kosinus-Funktionen zerlegen.\\
		Hierfür müssen folgende Bedingungen gelten (Dirichlet-Bedingungen):
		\begin{enumerate}
			\item Anzahl der Unstetigkeiten innerhalb einer Periode ist endlich
			\item Anzahl der Maxima und Minima innerhalb einer Periode ist endlich
			\item Funktion ist in jeder Periode integrierbar (d.h. die Fläche unter dem Betrag der Funktion ist in jeder Periode endlich)
		\end{enumerate}
		Die Sinus- und Kosinus-Funktionen werden auch Basisfunktionen genannt und bilden einen Vektorraum.
		\subsubsection{Fourier-Transformation Grundideen}
		Nachfolgend werden die Grundideen der Fourier-Transformationen erläutert.\\
		Die Ausgangsidee ist dass jedes Signal durch eine Summe von phasenverschobenen Kosinus-Funktionen beschreiben werden kann. Wir sprechen von einer Phasenverschiebung falls zwei Kosinus-Funktionen unterschiedliche Nullstellen haben (d.h. eine Verschiebung auf der x-Achse). Hierfür ergibt sich folgende Formel: $s[i] = \sum_{k=0}^{N/2} M_{k}*cos(2*\pi*k*i/N+\phi_{k})$ wobei $i = 0,\dots,N-1$, $M_{k}$.\\
		Die zweite Idee ist dass jede phasenverschobene Kosinus-Funktion $M*cos(x+\phi)$ kann durch eine Summe von Kosinus- und Sinus-Funktion ohne Phasenverschiebung repräsentiert werden: $M*cos(x+\phi) = A*cos(x) + B*sin(x)$ mit $A=M*cos(\phi)$ und $B=M*sin(\phi)$ wobei $M$ ist die Amplitude und $\phi$ die Phasenverschiebung. Dies erhalten wir durch den Übergang von Kartesischen-Koordinaten in Polar-Koordinaten in der komplexen Zahlenebene. Dadurch bestehen die  Polarkoordinaten  aus einen Imaginär und einen Realteil.\\
		Da wir uns im diskreten Bereich befinden gilt folgende Eigenschaft: Das Signal aus N Werten ist ein N-dimensionaler Vektor. Wie vorher beschrieben bilden die gesuchten Basisfunktionen einen Vektorraum. Für einen N-dimensionalen Vektorraum benötigen wir also eine Basis mit N orthogonalen Vektoren (d.h. das Skalarprodukt zweier Basis-Vektoren muss 0 sein).\\
		Die diskreten Sinus und Kosinus-Funktionen $c_{k}[i] = sin(2*\pi*k*i/N)$ bzw. $c_{k}[i] = cos(2*\pi*k*i/N)$ sind alle zueinander orthogonal. Da die Summe von 0 bis $N/2$ läuft erhalten wir genau $N/2+1$ phasenverschobene Kosinus-Funktionen, welche jeweils in eine Sinus und eine Kosinus Funktion zerlegt wird. Somit erhalten wir $2*(N/2+1) = N+2$ Basisfunktionen. Da $s_{0} = sin(0)$ und $s_{N/2} = sin(2*pi*N/2*i/N) = sin(\pi*i)$ jeweils Nullvektoren sind, können diese Verworfen werden wodurch wir $N$ Basisfunktionen erhalten.\\
		Unter einer diskreten Fourier-Transformation verstehen wir die Transformation des Signalsvektors in ihre Sinus- und Kosinus-Basis.
		\subsubsection{Diskrete Fourier-Transformation}
		Man unterscheidet bei der diskreten Fourier-Transformation zwischen der Zeit-Domäne und der Frequenz-Domäne. Der Übergang von der Zeit-Domäne in die Frequenz-Domäne wird Diskrete-Fourier-Transformation (DFT) genannt. Der rückwärtige Übergang wird Invers-Diskrete-Fourier-Transformation (IDFT) genannt.\\
		Die Zeit-Domäne $x[]$ besteht aus N-Samples, welche von 0 bis N-1 nummeriert sind. Die Frequenz-Domäne beinhaltet die durch die DFT erhaltenen Real- $ReX[]$ und Imaginär-Teile $ImX[]$, welche jeweils aus $N/2+1$ Elementen besteht. Die Real-Teile beschreiben Amplituden der Kosinus-Wellen, wobei die Imaginär-Teile die Amplituden der Sinus-Wellen bechreiben.\\
		Die Basisfunktionen d.h. die Funktionen die ein Signal x zerlegen:
			$$c_{k}[i]=cos(2*\pi*k*i/N)$$
			$$s_{k}[i]=sub(2*\pi*k*i/N)$$
		wobei k die Wellenzahl ist. $c_{k}$ bzw. $s_{k}$ ist das Signal der Kosinus- bzw. Sinusfunktion die mit der Amplitude in der Fourierzerlegung auftritt. Alle Basisfunktionen müssen genau so lang wie das Signal sein. Der Parameter $k$ gibt die Anzahl der Zyklen innerhalb der Signallänge an. $c_{0} = Re[0]$ ist der Gleichstrom-Versatz (DC-Offset). $s_{0} = Im[0]$ und $s_{N/2} = Im{N/2}$ sind überall 0, also irrelevant für die gesuchte Basis.\\
		Bisher wissen wir welche Basisfunktionen in dem Signal enthalten seien können. Jedoch fehlt uns der Anteil der Basisfunktion in dem Ausgangssignal d.h. uns fehlt noch die konkrete Berechnung der Real- bzw. Imaginar-Teile (Amplituden der Sinus- bzw. Kosinus-Funktionen). Bevor wir uns damit befassen ist noch eine Vorüberlegung notwendig.\\
		Da wir wissen wollen wie ähnlich unser Eingangssignal zu unser Basis ist berechnen wir nachfolgend die Korrelation. Die allgemeine Formel ergibt sich wie folgt:
		$$ \sum_{i=0}^{N-1} x[i]*y[i]$$ mit $x$ als Eingangssignal und $y$ unsere Basis.\\
		Ausgehend von dieser Beobachtung und unseren vorher ermittelten Basisfunktionen ergeben sich nachfolgend die Formeln für die Amplituden:
		$$ReX[k] =\sum_{i=0}^{N-1} x[i]*cos(2*\pi*k*i/N) $$ bzw. $$ImX[k] =\sum_{i=0}^{N-1} x[i]*sin(2*\pi*k*i/N) $$ für $k = 0,\dots,N/2$.
		\subsection{TODO}
		
\section{Merkmale}
	\subsection{Statistische Merkmale}
		\subsubsection{Erwartungswert}
		Der Erwartungswert einer diskreten Zufallsvariable $x$ mit den möglichen Werten $x_{i}$ und zugehörigen Wahrscheinlichkeiten $P(x_{i})$ ist: $E[x] = \sum_{i=1}^{n} (x_{i}*P(x_i))$.\\
		Der Erwartungswert einer kontinuierlichen Zufallsvariable $x$ mit Wertebreich $X$ und zugehöriger Wahrscheinlichkeitsdichte $p(x)$ ist: $E[x] = \int_{X} (x*p(x)) dx$.\\
		Der Erwartungswert einer Zufallsvariable wird auch als Mittelwert $\mu$ bezeichnet.\\
		Für gleichverteilte diskrete Zufallsvariablen x mit Werten $x_1,x_2,\dots,x_n$ gilt: $P(x_i) = 1/n$ und somit: $E[x] = \sum_{i=1}^{n} (x_{i}*\frac{1}{n}) = \frac{1}{n}*\sum_{i=1}^{n} x_{i}$.\\
		Für die Erwartungswerte von Funktionen einer Zufallsvariablen, $f(x)$ gilt: $E[f(x)]=\sum_{i=1}^{n} (f(x_i)*P(x_i))$ bzw. $E[f(x)] = \int_{X} (f(x)*p(x) dx$.
		\subsubsection{Momente}
		Der r-te Moment einer Zufallsvariable x ist $E[x^r]$. Der erste Moment mit $r=1: E[x] \mu$ heißt auch Mittelwert.\\
		Der r-te zentrale Moment ist: $\mu_r = E[(x-E[x])^r] = E[(x-\mu)^r]$.\\ 
		Das zweite zentrale Moment $\mu_2 = E[(x-\mu)^2] = \sigma^2$ heißt auch Varianz; $\sqrt{\sigma^2}$ heißt Standardabweichung $\sigma$.\\
		Die Schiefe einer Verteilung ist ein Maß für ihre Asymmetrie: $skew(x) = \frac{\mu_3}{\sigma^3} = \frac{E[(x-\mu)^3]}{\sigma^3}$. Wenn $skew(x) > 0$ linkssteil (rechtsschief) bzw. $skew(x) < 0$ rechtssteil (linksschief).\\
		Die Wölbung einer Verteilung ist ein Maß für ihrere Spitzheit: $kurt(x) = \frac{\mu_4}{\sigma^4}$. Wenn $kurt(x) < 3$ flach bzw. $kurt(x) > 3$ spitz. Falls x normalverteilt gitl: $kurt(x)=3$.
		\subsubsection{Emprische Werte}
		Die vorliegenden Messwerte $x_1,\dots,x_n$ stellen eine Stichprobe aus der Zufallsvariablen x zugrunde liegenden Verteilung dar. Überlichweise sind die wahren Parameter ($\mu,\sigma$) dieser Verteilung nicht bekannt. Mann muss daher diese Parameter auf Basis der Stichprobe schätzen.\\
		Der empirische Mittelwert: $\overline{x} = \frac{1}{n}*\sum_{i=1}^{n} x_i$ wobei $E[\overline{x}] = \mu$.\\
		Die empirische Varianz ist: $s^2 = \frac{1}{n-1}*\sum_{i=1}^{n}(x_i-\overline{x})^2$ wobei $E[s^2] = \sigma^2$.
		\subsubsection{Lageparameter}
		Lageparameter treffen allgemeine Aussagen über die Position der Verteilung und stellen in gewisser Weise die Lage ihres Schwerpunkts dar. Verschiedene Lagemaße sind dabei unterschiedlich robust, zeigen sich also mehr oder weniger empfindlich gegenüber Ausreißerwerten. Beispiele: Mittelwert, Median, Modus.\\
		Der Median ist der kleinste Wert $x_m$, bei dem die kumultative Verteiltungsfunktion $F_p(x) = \int_{-\inf}^{x} p(z) dz$ einen Wert von $\geq 0.5$ liefert d.h. er teilt die Verteilungsfunktion in zwei (flächenmäßig) gleich große Teile.\\
		Für eine geordnete Stichprobe $x_1\leq\dots\leq x_n$ ist der Median der Wert $x_{(n+1)/2}$ (falls n ungerade) bzw. der Wert $1/2*(x_{n/2}+x_{n/2+1})$. Daraus folgt dass der Median als Lagemaß wesentlich unempfindlicher ist gegenüber Ausreißer als der Mittelwert.\\
		Der Modus einer Verteilung ist der Wert mit der größten Wahrscheinlichkeit. Gibt es nur einen Modus nur einen Modalwert nennt man die Verteilung unimodal, sonst besitzt Verteilung  mehre  Modalwerte und nennt sie desshhalb bimodal.\\
		Es gilt:
		\begin{itemize}
			\item links-steile, rechts-schiefe Verteilung: Modus $<$ Median $<$ Mittelwert
			\item rechts-steile, links-schiefe Verteilung: Modus $>$ Median $>$ Mittelwert
		\end{itemize}
		\subsubsection{Streuungsmaße}
		Streuungsmaße beschreiben die Breite bzw. die Ausdehnung einer Verteilung und somit die Abweichung vom Schwerpunkt. Sie sind somit ein Maß für die Variabilität der Daten. Beispiel: Varianz, Quatile, Interquartilsabstand.
		\begin{itemize}
			\item Quartile: drei Quartile teilen die geordnete Datenmenge in vier gleich große Segmente, wobei das zweite Quartil gleichzeitig den Median darstellt
			\item Interquartilabstand bezeichnet den Abstand zwischen den 1. und den 3. Quartil und umfasst also die Hälfte der Daten und ist analog zum Median, robuster gegen Ausreißer als die Varianz-
		\end{itemize}	
		\subsubsection{Standardisierung}
		\begin{itemize}
			\item Problem: Normalverteilung $N(\mu,\sigma^2)$ hat nicht zwangsläufig eine Fläche von 1 unter der Kurve
			\item Lösung: Transformation mit $z = \frac{x-\mu}{sigm}$ dadurch Normalverteilung $N(0,1)$ als Ergebnis
			\item Mahalanobis-Abstand: $r=\frac{|x-\mu|}{\sigma}$ (Z-Score) (Musst die Distanz zwischen x und $\mu$ in Einheiten der Standardabweichung)
			\item wenn $z=\frac{x-\mu}{\sigma}$ dann gilt:
			\begin{itemize}
				\item $skew(x) = \frac{E[(x-\mu)^3]}{\sigma^3} = E[(\frac{x-\mu}{\sigma})^3]  = E[z^3]$
				\item $kurt(x) = \frac{E[(x-\mu)^4]}{\sigma^4} = E[(\frac{x-\mu}{\sigma})^4]  = E[z^4]$
				\item d.h. Schiefe und Wölbung von x sind das 3 bzw. 4 Moment der standardisierten Verteilung x
			\end{itemize}
			\item für multivarianten Fall mit $\Upsigma$ die Kozarianzmatrix gilt: $z=\Upsigma^{-1/2}*(x-\mu)$ d.h. aus $N(\mu,\Upsigma)$ wird $N(0,I)$
		\end{itemize}
		\subsubsection{Korrelation}
		\begin{itemize}
			\item x,y Zufallsvariablen und $\mu_x,\mu_y$ die Erwartungswerte und $\sigma_x,\sigma_y$ die Standardabweichung
			\item Korreleationkoeffizient $p_{xy}$ ist ein Maß für die lineare Abhängigkeit
			\item $p_{xy} = \frac{E[(x-\mu_x)*(y-\mu-y)]}{\sigma_x*\sigma_y}$
			\item wenn $|p_{xy}| = 1$ denn lineare Abhängigkeit zwischen x und y
			\item wenn x und y unabhängig $\rightarrow p_{xy} = 0$ anders rum nicht
			\item emprische Korreleation $r_{xy}$ zweier Stichproben $x=x_i,y=y_i$ misst die Abhängigkeit zweier Stichproben: $r_{xy} = \frac{x'*y'}{||x'||*||y'||}$ mit $x'_i = x_i-\overline{x},y'_i=y_i-\overline{y}$
		\end{itemize}
		\subsubsection{Einsatz statistischer Merkmale}
		\begin{itemize}
			\item problemunabhängig d.h. können immer eingesetzt werden da kein Vorwissen über die Problemstruktur 
		\end{itemize}
	\subsection{Merkmalstypen}
		\begin{center}
			\begin{tabular}{|c|c|c|c|}
				\hline Niveau & Operationen & Lageparameter & Beispiel \\ 
				\hline nominal & $\{=,\neq\}$ & Modus &  Geschlecht\\ 
				\hline ordinal & $+\{>,<\}$ & +Median  & Bundesligatabelle \\ 
				\hline intervall & $+\{-,+\}$ & +Erwartungswert &  Geburtsjahr\\ 
				\hline ratio & $+\{/,*\}$ & +geom. Mittel & Wohnfläche \\ 
				\hline 
			\end{tabular} 
		\end{center}
		\begin{itemize}
			\item Intervall und Ratio Saklen sind metrisch d.h. Abstandsbegriff ist sinnvoll
			\item Addition auf Intervallsaklen für Abstände sinnvoll
			\item Ratioskalen haben Nullpunkt
			\item Nominal- und Ordinalskalen sind immer diskret
		\end{itemize}
	\subsection{Merkmale für Zeitreihen}
		\subsubsection{Summarische Merkmale}
		\begin{itemize}
			\item Gegeben ein Signal $x = x_1,\dots,x_n$
			\item Zero Crossing Rate (im Ortsbereich): $zcr(x) = \frac{1}{n-1} \sum_{i=2}^{n} \mathfrak{I}\{x_i*x_{i-1} < 0\}$ mit $\mathfrak{I}\{A\} = 1$ falls A wahr
			\item Energie (im Orts- und im Frequenzbereich): $en(x) = \sum_{i=1}^{n} x_i^2$
			\item Entropie (im Orts- und im Frequenzbereich); $ent(x) = -\sum_{i=1}^{n} x_i^**log(x_i^*)$ wobei $x_i^* = \frac{x_i}{\sum_{j=1}^{n}x_j}$
			\item Schwerpunkt (spectral centroid)(Frequenzbereich): $sc(x) = (\sum_{k=1}^{K}f_k*|x_k|^2)/(\sum_{k=1}^{k}|x_k|^2)$
			\item Bandbreite (bandwidth)(Frequenzbereich) $bw(x) = (\sum_{k=1}^{k}(f_k-sc)^2*|x_k|^2)/(\sum_{k=1}^{k}|x_k|^2)$
		\end{itemize}
		\subsubsection{Autokorrelation, Grundfrequenz}
		\begin{itemize}
			\item Signal $x = x_1,\dots,x_n$
			\item Autokorrelation: $R_x(\tau) = (x\star x)(\tau) = \sum_{i}^{n} x_i*x_{i-\tau}$
			\item $\tau$ = Verzögerungsparameter (Lag)
			\item üblich Autokorrelation: $ACF_x(\tau) = \frac{R_x(\tau)}{R_x(0)}$
			\item Verwendung: Bestimmung der Grundfrequenz $->$ das zweite Maximum der ACF liefert die $1/f_0$ Periodendauer
			\item d.h. Tau als Parameter $\rightarrow$ durchprobieren bis zweite Maximum gefunden, Begründung: $\tau = 0$ ist immer das erste Maxima
			\item Maximum des Fourierspektrums nicht geeignet da im niedriegen Frequenzbereich nur eine grobe Auflösung
		\end{itemize}
		\subsubsection{Phasendifferenz}
		\begin{itemize}
			\item zwei Sinus Signale: $x_i = sin(w*i), y_i = sin(w*i+\phi)$ d.h. gleiche Frequenz aber Phasenverschiebung
			\item $\phi = arccos(r_{xy})$
		\end{itemize}
	\subsection{Merkmale für kinetische Systeme ????}
\section{Bayessche Entscheidungstheorie}
	\subsection{Einfürhung}	
	\begin{itemize}
		\item Natur hat einen Zustand $\omega$ aus einer Wertemenge $\Omega = \{\omega_1,\omega_2,\dots\}$
		\item Beispiel $\omega$: Fisch ist Wolfsbarsch ($\omega_1$) oder Lachs($\omega_2$) 
		\item $\omega$ ist meistens unbekannt d.h. eine Zufallsvariable
		\item Wenn es gleich viele Lachse und Wolfsbarsche gibt, ist es gleichwahrscheinlich die eine oder andere Art zu tippen
		\item a priori Wahrscheinlichkeit = Vorwissen
		\item P("Barsch") + P("Lachs) = 1
		\item wenn wir nur die a priori Wahrscheinlichkeitne kennen ist die Wahl mit der größten Wahrscheinlichkeit immer die beste
		\item Typischerweise haben wir mehr Informationen d.h. zusätzlichen Wissen für unsere Entscheidung
		\item Beispiel: die Helligkeit (Merkmal $x \in \mathbb{R}$) hängt von der Fischart $\omega$ ab d.h. wir haben klassenabhängige Wahrscheinlichkeiten
		\item $p(x|\omega)$ ist die klassenabhängige Wahrscheinlichkeit des Merkmals x 
		\item uns interessiert $P(\omega_i|x)$ also die Wahrscheinlichkeit dass die Welt den Zustand $\omega_i$ hat nachdem wir x beobachtet haben
		\item Verwendung: Satz von Bayes: $P(\omega_i|x) = \frac{p(x|\omega_i)*P(\omega_i)}{p(x)}$ mit $p(x) = \sum_{i=1}^{2} p(x|\omega_i) P(\omega_i)$ (für unseren Fall von zwei Fischen)
		\begin{itemize}
			\item $P(\omega_i|x)$: Die Wahrscheinlichkeit des wahren Zustands, nachdem wir den Beweis haben (Posteriori)
			\item $p(x|\omega_i$: Die Passfähigkeit des Beweises x zum möglichen wahren Zustand $\omega$ (Likelihood)
			\item $P(\omega_i$): Die Wahrscheinlichkeit des wahren Zustands bevor wir den Beweis gesehen haben
		\end{itemize}
		\item Nun Entscheidung für $\omega$ mit der größten Posteriori $P(\omega_i|x)$ 
	\end{itemize}
	\subsection{Bayes’sche Entscheidungstheorie}
		\subsubsection{Definitionen}
		\begin{itemize}
			\item Merkmalsvektor x als Element eines d-dimensionalen Euklidischen Merkmalsraums $R^d$ d.h. $x \in R^d$
			\item $\Omega = \{\omega_i,\omega_2,\dots,\omega_c\}$ eine endliche Menge von möglichen Zuständen (Klassen, Kategorien) 
			\item $A = \{\alpha_1,\dots,\alpha_a\}$ eine endliche Menge von möglichen Aktionen
			\item Eine Kostenfunktion $\Lambda(\alpha_i|\omega_j)$ die angiebt welche Kosten die Aktion $\alpha_i$ verursacht wenn der Zustand $\omega_j$ ist
		\end{itemize}
		\subsubsection{Bayes’sche Entscheidungstheorie - formal}
		\begin{itemize}
			\item $p(x|\omega_j)$ und $P(\omega_j)$ sind wie vorher die klassenbedingte Wahrscheinlichkeit von x gegeben $\omega_j$ bzw. die a-priori Wahrscheinlichkeit von $\omega_j$
			\item Es gilt analog für die a-posteriori-Wahrscheinlicht: $p(\omega_j|x) = \frac{p(x|\omega_j)*P(\omega_j)}{p(x)}$ wobei $p(x) = \sum_{i=1}^{c} p(x|\omega_i)P(\omega_i)$
			\item gegeben ein Merkmalsvektor x. Nun soll eine Aktion $\alpha_i$ durchzuführen. Welche kosten entstehen dabei?
			\item Wenn Zustand $\omega_j$ ist sind die Kosten von $\alpha_i$ nach Def: $\Lambda(\alpha_i|\omega_j)$
			\item Zustand $\omega_j$ ist jedoch nicht gegeben, wir kennen nur $P(\omega_j|x)$. Können aber den Erwartungswert der Kosten d.h. das bedingte Risiko $R(\alpha_i|x)$ bestimmen: $R(\alpha_i|x) = E_{\omega|x}[\Lambda(\alpha_i|\omega)] = \sum_{j=1}^{c} \Lambda(\alpha_i|\omega_j)P(\omega_j|x)$
			\item Bayessche Entscheidungsregel besagt nun: wenn x gegeben, wähle Aktion $\alpha_i$ für die das bedingte Risiko $R(\alpha_i|x)$ minimal ist
			\item sei $\alpha(x)$ eine Entscheidungsregel welche für ein gegebenes x eine Aktion $\alpha_i$ auswählt
			\item dann ist das Gesamtrisiko unter dieser Entscheidungsregel: $R_\alpha = \int R(\alpha(x)|x)p(x) dx$
			\item $R_\alpha$ wird minimal wenn man für jeden Punkt x das Minimum wählt d.h. die Aktion $\alpha_i$ für die $R(\alpha_i|x)$ minimal ist
			\item das minimale unvermeidbare Risiko $R^*$ heißt Bayes-Risiko
		\end{itemize}
		\subsubsection{Anwendung}
		\begin{itemize}
			\item ???
		\end{itemize}
		\subsubsection{Klassifikation und Diskriminanten}
		\begin{itemize}
			\item Klassifikatoren können durch Diskriminantenfunktionen repräsentiert erden
			\item Klassifikator für die Klassen $\omega_1,\dots,\omega_c$ besteht aus einer Menge von Diskriminantenfunktionen $g_i(x),i\in\{1,\dots,c\}$
			\item dieser Klassfikator weisst x einer Klasse $\omega_i$ gdw. $g_i(x) > g_j(x), \forall j\neq i$
			\item Bayes-Klassifikator für den allgemeinen Fall mit Risiko kann wie folgt definiert werden: $g_i(x) = -R(\alpha_i|x)$
			\item für die Klassifikation mit minimalen Fehler vereinfacht sich dies zu: $g_i(x) = P(\omega_i|x) \propto p(x|\omega_i)P(\omega_i)$
			\item für eine gegebene Menge von Diskriminanten $g_i(x)$ liefert eine Transformation mit einer monoton wachsenden Funktion $f()$ eine äquivalente Menge von Diskriminanten $f(g_i(x))$
			\item dadurch alternative Formulierungen:
			\begin{itemize}
				\item $g_i(x) = P(\omega_i|x) = \frac{p(x|\omega_i)*P(\omega_i)}{p(x)}$
				\item $g_i(x) =p(x|\omega_i)*P(\omega_i)$
				\item $g_i(x) = ln(p(x|\omega_i))*ln(P(\omega_i))$
			\end{itemize}
			\item Ein Klassifikator (Entscheidungsregel) für $c$ Klassen zerlegt den Merkmalsraum $\mathbb{R}^d$ in maximal c Entscheidungsregionen $R_i,i\in \{1,\dots,c\}$
			\item Falls für einen Merkmalsvektor $x$ gilt $g_i(x) > g_j(x),\forall j\neq i$ dann liegt x in der Entscheidungsregion $R_i$ und x wird als $\omega_i$ klassifiziert
			\item verschiedene Entscheidungsregionen werden durch Entscheidungsgrenzen voneinander getrennt
			\item Entscheidungsgrenzne sind die Bereiche in denen die beiden größten Diskriminanten denselben Wert annehmen
		\end{itemize}
		\subsubsection{Normalverteilung}
		\begin{itemize}
			\item Formel (univariant): $p(x) = N(x;\mu,\sigma^2) = \frac{1}{\sqrt{2*\pi}\sigma}\exp[-\frac{1}{2}(\frac{x-\mu}{\sigma})^2]$
			\item Mittelwert (erster Moment): $\mu$, Varianz (zweite zentrale Moment): $\sigma^2$
			\item Der zentrale Grenzwertsatz: Die Summe von n unabhängigen Zufallsvariablen kovergiert gegen Normalverteilung mit $n\rightarrow \infty$. Da viele natürliche Vorgänge einer großen Zahl unabhängiger Störfaktoren unterliegen, ist die Normalverteilungsdannahme sinnvoll.
			\item hat hohe Entropie d.h. die Verteilung mit der größten Unsicherheit über die Werte einer Zufallsstichprobe
			\item Formel (multivariant): $N(x;\mu,\Sigma) = \frac{1}{(2*\pi)^{d/2}|\Sigma|^{1/2}}*\exp[-\frac{1}{2}*(x-\mu)^t\Sigma^{-1}(x-\mu)]$ 
			\begin{itemize}
				\item $x = (x_1,\dots,x_d)^t$ ein d-dimensionaler Spaltenvektor
				\item $\mu = (\mu_1,\dots,\mu_d)^t$ ein d-dimensnaler Mittelwert-Vektor
				\item $\Sigma$ ist eine $d\times d$ Kovarianzmatrix, $|\Sigma|$ die Diskriminante, $\Sigma^{-1}$ Inverse Matrix
				\item $\mu = E[x] = \int x*p(x)) dx$ (können Komponentenweise bestimmt werden $\mu_i = E[x_i]$)
				\item $\Sigma = E[(x-\mu)(x-\mu)^t] ) = \int (x-\mu)(x-\mu)^tp(x)dx$ ($\sigma_{ij} = E[(x_i-\mu_i)(x_j-\mu_j)]$)
			\end{itemize}
			\item für Kovarianzmatrix $\Sigma$ gilt:
			\begin{itemize}
				\item $\sigma_{ii}$ sind die Varianzen der entsprechenden $x_i$ also $\sigma^2_i$
				\item $\sigma_{ij}$ sind Kovarianzen von $x_i$ und $x_j$, Wenn $x_i$ und $x_j$ unabhängig dann gilt $\sigma_{ij} = 0$
				\item falls alle nicht-diagonalelemente Null sind, ist die multivariante Verteilung einfach das Produkt der d univarianten Verteilungen der Komponenten von x
				\item die Kovarianzmatrix ist symmetrisch und positiv (semi-) definit (M positiv definit: $x^tMx>0 \forall x \neq0$)
			\end{itemize}
			\item Stichproben aus multivarianter Gaußverteilungen bilden typischerweise eine Häufung deren Mittelpunk von $\mu$ und deren Form von $\Sigma$ definiert wird
			\item Orte gleicher Wahrscheinlichkeitsdichte liegen auf Hyperellipsoiden für die $(x-\mu)^t\Sigma^{-1}(x-\mu)$ konstant ist
			\item die Hauptachsen der Hyperellipsoide sind die Eigenvektoren von $\Sigma$, die Eigenwerte geben die Länge dieser Achsen an
			\item $r^2 = (x-\mu)^t\Sigma^{-1}(x-\mu)$ ist die quadrierte Mahalanobis-Distanz; Konturen konstanter Dichte sind Hpyerellipsoide konstanter Mahalanobis-Distanz zu $\mu$
		\end{itemize}
	\subsection{Diskriminanten für die Normalverteilung}
	\begin{itemize}
		\item vorher gezeigt dass Diskriminante $g_i(x) = ln(p(x|\omega_i))*ln(P(\omega_i))$ Klassifikation mit minimaler Fehlerrate liefert
		\item wenn $p(x|\omega_i) = N(\mu_i,\Sigma_i)$ einsetzen in $g_i(x)$ : $g_i(x) = -\frac{1}{2}(x-\mu_i)\Sigma_i^{-1}(x-\mu_i)-\frac{d}{2}ln(2\pi)-\frac{1}{2} ln(|\Sigma_i|)+ln(P(\omega_i))$ $\rightarrow$ Entfernung von $-\frac{d}{2}ln(2\pi)$ da konstant und nicht von der Klasse abhängt
		\item Also erhalten wir: $g_i(x)$ : $g_i(x) = -\frac{1}{2}(x-\mu_i)^t\Sigma_i^{-1}(x-\mu_i)-\frac{1}{2} ln(|\Sigma_i|)+ln(P(\omega_i))$
		\item Entscheidungsgrenzen:
		\begin{itemize}
			\item $\Sigma_i = \sigma^2I$ d.h. Alle Merkmale unabhängig, gleiche Varianzen, auf der Hauptdiagonalen stehen die gleichen Werte; Die Klassen bilden kugelförmige Häufungen gleicher Größe um ihre jeweiligen Mittelpunkte.
			\begin{itemize}
				\item es gilt: $\Sigma_i^{-1} = (1/\sigma^2)I$ und $|\Sigma_i| = \sigma^{2d}$
				\item einsetzen in Formel: $g_i(x) = -\frac{1}{2\sigma^2}(x-\mu_i)^t(x-\mu_i)-\frac{1}{2} ln(\sigma^{2d})+ln(P(\omega_i))$ 
				\item Konstanten entfernen: $g_i(x) = -\frac{1}{2\sigma^2}(x-\mu_i)^t(x-\mu_i)+ln(P(\omega_i))$ 
				\item Euklidische Distanz: $(x-\mu_i)^t(x-\mu_i) = ||x-\mu_i||^2$
				\item Also:  $g_i(x) = -\frac{||x-\mu_i||^2}{2\sigma^2}+ln(P(\omega_i))$ 
				\item Expandieren:  $g_i(x) = -\frac{1}{2\sigma^2}(x^tx-2\mu_i^tx+\mu_i^t\mu)+ln(P(\omega_i))$ 
				\item Also erhalten wir eine lineare Diskriminante: $g_i(x) = w_i^t*x+ w_{i0}$ (lineare Maschine, $w_{i0}$ ist Schwellwert)
				\item d.h. die Entscheidungsgrenzen zwischen den Regionen werden durch die lineare Gleichung: $g_i(x) = g_j(x)$ definiert (Hyperebenen)
				\item die Hyperebene ist orthogonal zum Vektor w, der Verbindungsline zwischen den beiden Mittelwertvektoren
				\item Line wird in $x_0$ geschnitten falls die a-priori-Wahrscheinlichkeiten gleich sind, liegt $x_0$ zwischen $\mu_i$ und $\mu_j$
			\end{itemize}
			\item $\Sigma_i = \Sigma$ d.h. alle Klassen haben gleiche Kovarianzmatrix d.h.Die Klassen bilden hyperellipsoide Häufungen gleicher Größe, Form und Orientierungen um ihre jeweiligen Mittelpunkte.
			\begin{itemize}
				\item wieder einsetzen in $g_i$: $g_i(x) = -\frac{1}{2}(x-\mu_i)^t\Sigma_i^{-1}(x-\mu_i)-\frac{1}{2} ln(|\Sigma_i|)+ln(P(\omega_i))$ 
				\item Konstanten entfernen:  $g_i(x) = -\frac{1}{2}(x-\mu_i)^t\Sigma_i^{-1}(x-\mu_i)+ln(P(\omega_i))$
				\item Ausmultiplizieren und Symmetrie von $\Sigma$ liefert wieder lineare Maschine 
				\item nicht mehr orthogonal zur Verbindungslinie der Mittelwerte, bei gleicher a-priori-Wahrscheinlicht $x_0$ immer noch auf halben Wege zwischen $\mu_i$ und $\mu_j$
			\end{itemize}
			\item $\Sigma_i = beliebig$
			\begin{itemize}
				\item einsetzen und ausmultiplizieren ergibt Quadratische-Diskriminante
				\item beliebige Normalverteilungen führen zu Entscheidungsgrenzen welche allgemeine Hyperquadriken sind
				\item Bereits kleine Anzahl von Klassen können die Entscheidungsgrenzen komplexe Formen annehmen lassen
			\end{itemize}
		\end{itemize}
	\end{itemize}
	\subsection{Empirischer Fall}
	\begin{itemize}
		\item im Anwendungsfall eher unrealisitisch das man alle Parameter kennt d.h. muss diese auf Basis von Traningsdaten schätzen
		\item wenn man Normalverteilung annehmen möchte:
		\begin{itemize}
			\item $\hat{P}(\omega_i) = \frac{n_i}{n}$
			\item $\hat{\mu}_i = \frac{1}{n_i}\sum_{j=1}^{n_i} x_{ij}$
			\item $\hat{\Sigma} = \frac{1}{n-c}\sum_{i=1}^{c}\sum_{j=1}^{n_i}(x_{ij}-\hat{\mu}_i)(x_{ij}-\hat{\mu}_i)^t$
			\item mit $n_i$ Anzahl der Traningsdatensätze für die Klassei, n Anzahl aller Traningsdatensätze und $x_{ij}$ Traningsdatensatz Nummer j für Klasse i
			\item dies nennt man Lineare Diskriminazanalyse
		\end{itemize}
		\item für Fall 3:
		\begin{itemize}
			\item $\hat{P}(\omega_i) = \frac{n_i}{n}$
			\item $\hat{\mu}_i = \frac{1}{n_i}\sum_{j=1}^{n_i} x_{ij}$
			\item $\hat{\Sigma_i} = \frac{1}{n_i-1}\sum_{j=1}^{n_i}(x_{ij}-\hat{\mu}_i)(x_{ij}-\hat{\mu}_i)^t$
			\item dies nennt man Quadratische Diskriminazanalyse
		\end{itemize}
		\item Bias/Variance Tradeoff = ???
	\end{itemize}
	\subsection{Diskrete Merkmale}
	\begin{itemize}
		\item Integrale durch Summen ersetzen, sonst alles wie bisher
	\end{itemize}
\section{Parameterschätzung}
	\subsection{Einführung}
	\begin{itemize}
		\item Ziel: Konstruieren eines Klassifikators
		\item Benötigen: a-priori-Verteilung $P(\omega_i)$ und klassenbedingten Verteilungen $p(x|\omega_i)$
		\item Problem: Verteilungen sind unbekannt
		\item haben Anzahl an Stichproben $D=(x_1,\dots,x_n)^t$ mit dazugehörigen Klassen $t=(\omega_{i1},\dots,\omega_{\omega_{in}})^t$ mit D ist eine $n \times d$ Matrix, d ist die Anzahl der Dimensionen des Merkmalsraums. Jede Zeile von D ist eine Beobachtung $x_i$
		\item $n_i$ Anazahl der Trainingsdaten der Klasse $\omega_i$ und es gilt $n = \sum_{i=1}^{c} n_i$
		\item $P(\omega_i)$ können wir schwätzen durch z.b. Zahlen der Häufigkeiten: $\hat{P}(\omega_i) = \frac{n_i}{n}$
		\item $p(x|\omega_i)$ ist das Problem wir müssen eine Funktion $f(x)$ schätzen d.h. den Funktionswert an unendlich vielen Stellen $x\in R^d$, Problem: Raumvolumen wächst exponentiell $\rightarrow$ bei einer festen Anzahl von Trainingsdaten sind $x_i$ immer weiter im Raum versteht d.h. die leeren Bereiche welche $f(x)$ raten muss werden immer größer
		\item Lösung: Annahme das $f(x)$ keine beliebige Form hat sondern aus einer Funktionsfamilie $f(x|\theta)$ stammt, aus der die gescuhte Funktion durch einen Parametervektor $\theta$ mit niedriger Dimensionaltiäöt bestemmt werden wird
		\item Beispiel: $f(x|\theta) = N(x|\mu,\Sigma)$ mit $\theta = (\mu,\Sigma)$ d.h. wir müssen nicht mehr unendlich viele Werte aus R schätzen sondern nur noch wenige Parameter
		\item Aufgabe: Auf Basis der Annahme der parametrischen Form $p(x|\theta_i)$ der Verteilungsfunktion $p(x|\omega_i)$ bestimme den Parametervektor $\theta_i$ mit Hilfe der Traningsdaten $D^i,t^i$
		\item Lösungen:
		\begin{itemize}
			\item Maximum Likelihood (Punktschätzung)
			\item Bayes'sch (Bestimmung der a-posteriori Verteilung von $\theta$)
		\end{itemize}
		\item Also: Wir haben Trainingsdaten $D = x_1,\dots,x_n$ und wir nehmen an, dass $x_k$ aus einer parametrischen Verteilung $p(x_k|\theta)$ stammen. Gesucht ist nun eine Schätzung für die wahren Parameter die am besten mit den Traningsdaten vereinaber ist
	\end{itemize}
	\subsection{Maximum Likelihood-Schätzer}
	\begin{itemize}
		\item Annahme: der Parametervektor $\theta$ hat einen wohldefinierten festen Wert der uns unbekannt ist
		\item Maximum-Likelihood-Schätzer: $\hat{\theta} = argmax_{\theta \in \Theta} p(D|\theta$) also derjenige Wert $\hat{\theta}$ der die Likelihood der Traningsdaten maximiert, Anwendung Satz von Bayes: $p(\theta|D) = \frac{p(D|\theta)*p(\theta)}{p(D)}$
		\item Frage: Wo kommt der her? Man erwartet eigentlich $p(\theta|D)$
		\item Antwort: eigentlich interessiert uns $p(\theta|D)$ aber uns interessiert nicht die gesamte Verteilung sondern nur der Wert $argmax_\theta p(\theta|D)$
		\item Annahme: Alle Werte von $\theta$ sind gleich wahrscheinlich $p(\theta) = c$
		\item also $argmax_\theta p(\theta|D) = argmax_\theta \frac{p(D|\theta)*p(\theta)}{p(D)} = argmax_\theta \frac{p(D|\theta)*c}{c'} = argmax_\theta p(D|\theta) = \hat{\theta}_{ML}$
		\item Gegeben:
		\begin{itemize}
			\item Menge von Daten: $D = x_1,x_2,\dots,x_n$
			\item parametrische Verteilungsfunktion: $p(x|\theta)$
			\item p-dimensionalen Parametervektor: $\theta = (\theta_1,\dots,\theta_p)^t$
		\end{itemize}
		\item Gesucht: $\hat{\theta}$ der $p(D|\theta)$ maximiert
		\item Annahme: $x_k$ sind unabhängige Stichproben aus $p(x|\theta)$ d.h. es gilt: $p(D|\theta) = \prod_{k=1}^{n}p(x_k|\theta)$
		\item Vorgehen:
		\begin{enumerate}
			\item Bestimme die Log-Likelihood-Funktion: $l(\theta) = log \prod_{k=1}^{n}p(x_k|\theta) = \sum_{k=1}^{n}log(p(x_k|\theta))$
			\item Wende Gradientenoperator $\nabla_\theta = (\frac{\partial}{\partial \theta_1},\dots,\frac{\partial}{\partial \theta_p})^t$ auf $l(\theta)$ an d.h. bestimme: $\nabla_\theta l = \sum_{k=1}^{n} \nabla_\theta ln(p(x_k|\theta))$
			\item Löse die Gleichung: $\nabla_\theta l = 0$
			\item Lösung der Gleicung ist $\hat{\theta}$
		\end{enumerate}
		\item Bias: Der Erwartungswert der Schätzwerte entspricht nicht den realen Parametern. ML-Schätzer sind jedoch asymptotisch erwartungstreu
		\item Maximum-Likelihood ist sehr einfach anzuwenden
		\item Wenn die a-posteriori Verteilung unimodal ist, dann liefert ML Ergebnisse die ähnlich gut sind wie Bayessche Schätzung, Ml kovergiert mit $n \rightarrow \infty$ gegen die wahren Parameter
	\end{itemize}
	\subsection{Bayessche Parameterschätzung}
	\begin{itemize}
		\item Aufgabe: Bestimme $p(x|D)$ anhand einer Stichprobe D die aus einer parametrischen Verteilung $p(x|\theta)$ gezogen wurde. Dabei habe $p(x|\theta)$ eine bekannte funktionale Form, nur $\theta$ ist nicht festgelegt
		\item Wir stellen uns $\theta$ als Zufallsvariable vor die eine a-priori-Verteilung (vorher $\theta$ unbekannt und fest jetzt unbekannt und aus einer Verteilung) $p(\theta)$ hat aus der wir eine a-posteriori-Verteilung $p(\theta|D)$ berechnen können
		\item es ergibt sich folgende a-posteriori Verteilung für die Daten: $p(x|D) = \int p(x|\theta)p(\theta|D) \partial\theta$
		\item Vergleich zu ML-Ansatz: $p(x|D) = p(x|\hat{\theta}_{ML})$ mit $\hat{\theta}_{ML} = arg max_\theta p(D|\theta)$
		\item Ziel: Bestimmung der Parameterverteilung $p(\theta|D)$
		\item Bayessche Satz: $p(\theta|D) = \frac{p(D|\theta)*p(\theta)}{\int p(D|\theta)p(\theta)\partial\theta}$ mit $p(D|\theta)$ ist Likelihood
		\item Annahme: Unabhängigkeitsannahme für D d.h. die Merkmale hängen nicht voneinander ab es gilt also: $p(D|\theta) = \prod_{k=1}^{n}p(x_k|\theta)$
	\end{itemize}
\section{Dimensionsreduktion}
	\subsection{Einführung}
	\begin{itemize}
		\item wenn Merkmale unabhängig sind dann lassen sich theoretisch Klassifizierer mit sehr hohrer Präzision realisieren
		\item für zwei-Klassen-Problem $p(x|\omega_i) = N(\mu_i,\Sigma)$ der Bayes-Fehler: $P(e) = \frac{1}{\sqrt{2*\pi}}\int_{r/2}^{\infty} e^{-u^2/2} du$ mit $r^2$ ist die quadrierte Mahalanobis-Distanz
		\item mit wachsendem r sinkt der Fehler, bei unabhängigen Merkmale ist $\Sigma = diag(\sigma_1^2,\dots,\sigma_d^2)$ 
		\item man erhält: $r^2 = \sum_{j=1}^{d} \frac{(\mu_1^{(j)}-\mu_2^{(j)})^2}{\sigma_j^2}$ d.h. je größer d (Anzahl der Dimensionen / Merkmale) desto besser ist die Klassifikation (theoretisch, praktisch eher nicht so)
		\item im Bayesschen Framework, wenn wir die Formen der Verteilungen kennen können wir die Anzahl der Features beliebig erhöhren und der Ansatz sorgt von selbst dass diese für irrelevante Features entsprechend gering gewichtet werden
		\item Probleme:
		\begin{itemize}
			\item in hohen Dimensionen sind bereit geringe Unterschiede zwischen wahrer und angenommener Verteilung sehr groß
			\item in hohen Dimensionen ist die Anzahl der Traningsdaten nicht mehr ausreichend um die erforderliche Anzahl der Parametern ausreichend zuverlässig zu schätzen
		\end{itemize}
		\item Lösung:
		\begin{itemize}
			\item Hauptkomponentenanalyse (PCA): Projektion in einem Raum mit reduzierter Dimensionalität die die Daten am besten repräsentiert
			\item Fisher-Diskriminante: Projektion in einem Raum mit reduzierter Dimensionalität in der die  Daten unterschiedlicher Klassen am besten getrennt sind
		\end{itemize}
	\end{itemize}
	\subsection{Hauptkomponentenanalyse}
	\begin{itemize}
		\item Aufgabe: reduziere die Anzahl der Dimensionen auf $q < d$
		\item Ansatz: finde einen q-dimensionalen Teilvektorraum $E^q$ mit Basis: $E = (e_1,\dots,e_q)^t$ so dass die Projektion der $x_k$ in den Raum $E^q$ möglichst gut repräsentiert sind d.h einen minimalen quadratischen Fehler verursacht
		\item Kriterium zu minimieren: $J(E) = \sum_{k=1}^{n}(E^t(Ex_k))-x_k)^2$
		\item $E$ ist eine $q\times d$ Matrix die Vektoren von einem d-dimensionalen Vektorraum $E^d$ in einen q-dimensionalen Untervektorraum $E^q$ abbildet
		\item $x'_k = Ex_k$ die Projekt einen d-dimensionalen Vektors $x_k$ in $E^q$
		\item $x''_k = E^tx'_k$ ist die Rücktransformation des reduzierten Vektors in den ursprünglichen Vektorraum, in dem der Fehler (Distanz zwischen $x''_k$ und $x_k$) gemessen
		\item q kann nicht größer als n sein da n Datenvektoren spannen höchstens n-dimensionalen Vektorraum auf
		\item Vorgehen:
		\begin{enumerate}
			\item Berechne empirische Kovarianzmatrix $\hat{\Sigma}$
			\item Berechne die Eigenwerte $\lambda_i$ und Eigenvektoren $e_i$ von $\hat{\Sigma}$
			\item wähle $q<d$ Eigenvektoren zu den größten Eigenwerten als dimensionsreduzierten Merkmalsraum mit Transformationsmatrix mit $E = (e_1,\dots,e_q)^t$
			\item Die neuen q-dimensionalen Merkmalsvektoren $x'$ ergeben sich jetzt durch die Transformation $x' = Ex$
		\end{enumerate}
	\end{itemize}
	\subsection{Diskriminantenanalyse}
	\begin{itemize}
		\item Aufgabe: Suche eine Transformation die das Verhältnis der Varianz zwischen den Klassen zur Varianz innerhalb der Klasse maximiert
		\item Die Transformation erzeugt Räume mit maximal $c-1$ Dimensionen da es nur c Klassen gibt, können c Klassenmittelpunkte einen Raum mit maximal $c-1$ Dimensionen aufspannen
		\item wir definieren:
		\begin{itemize}
			\item Streuung innerhalb der Klassen: $S_W =\sum_{i=1}^{c}\sum_{k=1}^{n_i}(x_{ik}-\mu_i)(x_{ik}-\mu_i)^t$
			\item Streuung zwischen den Klassen i: $S_B = \sum_{i=1}^{c}n_i(\mu_i-\mu)(\mu_i-\mu)^t$
			\item mit $x_{ik}$ ist Datensatz k zu Klasse i, $n_i$ ist die Anzahl der Datensätze für Klasse i, $\mu$ ist der Mittelwert überalle Klassen
		\end{itemize}
		\item Die Determinante einer Streuungsmatrix ist ein Maß für das Raumvolumen das die zugehörigen Daten benaspruchen
		\item Streungsverhältnis: $\frac{|S_B|}{|S_W|}$
		\item Gesucht: Transformation $W$ in einen $q \leq c-1$ dimensionalen Raum $E^q$ so dass das Streungsverhältnis in $E^q$ maximal wird, W ist eine $q \times d$ Matrix
		\item mit $W$ gegeben: $\hat{S}_B = \sum_{i=1}^{c}n_i(W\mu_i-W-\mu)(W\mu_i-W-\mu)^t = \dots = W*S_B*W^t$ analog: $\hat{S}_W = W*S_W*W^t$
		\item Aufgabe: minimiere $J(W) = \frac{|W*S_B*W^t|}{|W*S_W*W^t|}$
		\item Lösung: Eigenwertproblem (ausgelasssen) lösen
	\end{itemize}
	\subsection{Nichtlineare Diskiminaten}
	\begin{itemize}
		\item Gegeben: Feature Space welcher nicht linear trennbar ist
		\item Aufgabe: Finde einen Vektor $\phi = (\phi_1,\dots,\phi_f)$ von f (nichtlinearen) Transformationen $\phi_i(x)$ und betrachte lineare Diskriminante in f-dimensionalen Raum $E^f$
	\end{itemize}
\section{Nicht-parametrische Methoden}
	\subsection{Einführung}
	\begin{itemize}
		\item Bisher:
		\begin{itemize}
			\item vollständigen Wissen über die Verteilung: Bayessche Entscheidungsregel
			\item oder: wir kennen zumindestens die funktionale Form der Verteilung und können die Parameter der Verteilung aus den Trainingsdaten schätzen
		\end{itemize}
		\item Nun: Verteilungsfunktionen sind nicht bekannt
		\item Problemstellung:
		\begin{itemize}
			\item Gesucht: unbekannte Verteilung $p(x)$
			\item Gegeben: Menge von Stichproben $x_i \sim p(x)$ aus dieser Verteilung
			\item Idee: Dichte der $x_i$ in der Nähe von $x$ verwenden um damit auf die Wahrscheinlichkeit $p(x)$ zu schließen $\rightarrow$ je dichter die $x_i$ in einer Region liegen desto wahrscheinlicher dürfte es ja sein, Werte aus dieser Region zu erhalten vorrausgesetzt $p(x)$ ist nicht bösartig zerklüftet
		\end{itemize}
		\item Gegeben eine Region $R \subset R^n$ im Merkmalsraum und eine Verteilungsfunktion $p(x)$
		\item Wahrscheinlichkeit $P$ dass ein $x \in R^n$ in $R$ liegt dann: $P := Pr(x\in R) = \int_R p(x') dx'$
		\item nun seien n Stichproben $x_1,\dots,x_n$ mit $x_i \sim p(x)$ gegeben
		\item Wahrscheinlichkeit dass genau $k$ mit $k<n$ dieser Stichproben in R liegen: $P_k = \binom{n}{k} P^k (1-P)^{n-k}$ (binomialverteilt)
		\item Erwartungswert: $E[k] = \sum_k k*k_k = n*P$
		\item mit $E[k]$ lässt sich nun P umgekehrt schätzen: $P \approx \frac{k}{n}$
		\item Mit der Annahme dass $p(x)$ ist kontinuierlich und einer Umgebung von x nicht allzu stark schwanklt: Wenn V das Volumen der Region R ist dann Gilt: $k/n \approx P = \int_R p(x')dx' \approx \int_R p(x)dx' = p(x)\int_R 1dx' = p(x) = V$
		\item damit ergibt sich: $p(x) \approx \frac{k/n}{V}$
		\item Bedeutung: wir können $p(x)$ Abschätzen mit Hilfe einer Stichprobe der Größe n, wenn wir die Anzahl k der Samples in einer Region R um x in Relation zur Gestamtzahl zur Größe der Region V setzen
		\item zu zeigen: der Schätzwert kann die gesuchte Wahrscheinlichkeit $p(x)$ beliebig genau approximieren wenn man n nur groß genug wählt d.h.: $p(x) = \lim\limits_{n \rightarrow \infty} \frac{k_n/n}{V_n}$ wenn man $k_n$ und $V_n$ geeignet definiert
		\item damit $p(x) = \lim\limits_{n \rightarrow \infty} \frac{k_n/n}{V_n}$ gilt müssen folgende Bedingungen gelten:
		\begin{itemize}
			\item $\lim\limits_{n\rightarrow \infty} V_n  = 0$ d.h. der Fehler die Durchschnittsbilding im Raumvolumen verschwindet
			\item $\lim\limits_{n\rightarrow\infty} = \infty$ d.h. die möglicherweise irrationale Zahl P kann durch die Anzahl der Samples in R beliebig genau approximiert werden
			\item $\lim\limits_{n\rightarrow\infty} = 0$ mit schrumpfenden Raumvolumen sinkt auch die relative Anzahl von Treffern im Volumen
		\end{itemize}
		\item Zwei Ansätze:
		\begin{itemize}
			\item Parzen-Window-Ansatz (Kern-Dichte-Schätzer): Volumen $V_n$ in Abhängigkeit von n festlegen und dann zeigen dass Konvergenz mit $n\rightarrow\infty$ erreicht wird
			\item k-nearest-Neighbour: Anzahl der Treffen in Abhängigkeit von n festlegen und hierfür Konvergenz zeigen
		\end{itemize}
	\end{itemize}
	\subsection{Parzen Windows}
	\begin{itemize}
		\item Idee: um den Punkt x, dessen Wk wir bestimmen wollen, eine Testregion mit vorgegebene Volumen $V_n$ legen und dann zählen, welcher Anteil $k_n$ der Sichtprobe in dieses Volumen fällt
		\item das Verhältnis $k_n/n$ ist dann eine Schätzung für $p(x)$: Schätzung mit n Samples: $p_n(x)$
		\item Annahme: Testregion $R_n$ für n Stichproben ist Hyper-Cube mit Kantenlänge: $h_n$ dann gilt: $V_n = h_n^d$ d.h. Das Volumen ist die d-te Potenz der Kantenlänge (d = Dimonensionenzahl)
		\item Vorgehen:
		\begin{itemize}
			\item Fensterfunktion: $\phi(u) = \begin{cases}
			1 & |u_j| \leq 1/2; i = 1\dots,d\\
			0 & sonst
			\end{cases}$ (stanzt einem im Ursprung zentrierten Hyperwürfel der Kantenlänge 1 aus)
			\item für Hyperwürfel mit kantenlänge $h_n$ zentriert um x gilt: $\phi(\frac{x-x_i}{h_n}) = \begin{cases}
			1 & \text{falls} x_i \text{im Hyperwuerfel um x liegt}\\
			0 & sonst
			\end{cases}$
			\item $x_i$ die Punkte welche sich im Hypercube befinden
			\item Anzahl der Stichproben im Fenster: $k_n = \sum_{i=1}^{n}\phi(\frac{x-x_i}{h_n})$
			\item es folgt aus $p_n(x) = \frac{k_n/n}{V_n}$ und $k_n$: $p_n(x) = 1/n * \sum_{i=1}^{n} \frac{1}{V_n}\phi(\frac{x-x_i}{h_n})$
			\item $p_n(x)$ ist der Durchschnitt von n Funktionen die von x und den $x_i$ abhängen
		\end{itemize}
		\item Erweiterungen:
		\begin{itemize}
			\item $\phi(\frac{x-x_i}{h_n})$ kann unterschiedlich interpretiert werden: 1) Funktion von $x_i$ die ihren Wert in Anhängigkeit davon liefert ob $x_i$ im Fenster um $x$ liegt ($k_n$ = Anzahl der $x_i$ im Fenster um x) ODER 2) Funktion von x die ihren Wert in Abhängigkeit davon liefert ob x im Fenster um das jeweilige $x_i$ liegt ($k_n =$ Anzahl der Fenster um die $x_i$ in denen $x$ liegt)
			\item Andere Fenster
			\begin{itemize}
				\item Hyperwürfel mit 1/0 Zählung erzeugt Diskontinuität d.h scharfe Kanten
				\item besser glatte Fenster: Gewicht des Samples sinkt kontinuierlich mit der Entfernung
				\item Betrachtung von $\phi$ als Kern (Kernel) K: für K gilt: $\forall u: K(u) \geq 0; \int K(u) du = 1; \forall u: K(u) = K(-u)$
				\item Epanechnikov-Kern minimiert den erwarteten integrierten quadratischen Fehler zwischen geschätzter und wahrer Verteilung: $K_{ep}(u) = \begin{cases}
				\frac{d+2}{2*c_d}(1-||u||^2) & \text{falls} ||u||^2 < 1\\
				0 & \text{sonst}
				\end{cases}$
			\end{itemize}
		\end{itemize}
		\item Betrachtung als Faltung:
		\begin{itemize}
			\item $f_n(x') = \sum_{i=1}^{n} \delta(x'=x)$ d.h. eine Stichprobe wird durch $\delta$-Funktionen repräsentiert
			\item für einen Kern definieren wir: $K_h(x,x') = \frac{1}{n*h} K(\frac{x-x'}{h})$
			\item wir erhalten: $p_n(x) = K_h(x,x') \star f_n(x') = K_h(x,x') \star \sum_{i=1}^{n}\delta(x'=x) = \dots = \frac{1}{n*h}\sum_{i=1}^{n}K(\frac{x-x_i}{h})$
			\item d.h. Kern-Dichte-Schätzung ist die Filterung des Signals an einem Filter mit der Impulsantwort $K_h$  
		\end{itemize}
		\item Flexibilität:
		\begin{itemize}
			\item Parzen-Window Methode kann bei jeder Art von Verteilung angewendet werden
			\item Anzahl von Dimensionen Grenzen gesetzt: die Anzahl der erforderlichen Samples wächst exponentiell mit d d.h. $d > 3$ meist nicht mehr sinnvoll
		\end{itemize}
		\item Klassifikation:
		\begin{itemize}
			\item Bestimme $p(x|w_i)$ mit Hilfe der Parzen-Window-Methode
			\item Ordne einen neuen Punkt diejenigen Klasse zu, die die maximale a-posteriori-Wahrscheinlichkeit hat
			\item Seien $x_{ik},k=1,\dots,n_i$ die Samples der Klasse i
			\item klassenbedingte Wahrscheinlichkeit $\hat{p}(x|w_i) = \frac{1}{n_i*h_i}\sum_{k=1}^{n_i}K(\frac{x-x_{ik}}{h_i})$
			\item a-priori-Wahrscheinlichkeiten: $\hat{p}(w_i) = \frac{n_i}{n}$
			\item Klassifikation durch: $arg max_{w_i}\hat{p}(x|w_i)\hat{p}(w_i)$
			\item Kombination mit Naiven-Bayes-Klassifikator: Annahme dass die einzelnen Merkmale voneinander unabhängig sind: $p(x|w_j) = \prod_{i=1}^{d}p(x_i|w_j)$
			\item hierbei sind $p(x_i|w_j)$ eindimensionale Verteilungen die sich dann auch sinnvoll mit Hilfe des Parzen-Window-Ansatzes approximieren lassen
		\end{itemize}
		\end{itemize}
	\subsection{k Nearest Neighbours}
	\begin{itemize}
		\item Dichteschätzung:
		\begin{itemize}
			\item Ansatz: $p(x) = \frac{k}{n*V}$
			\item Kern-Dichte-Schätzer: Volumenfest bestimmt durch h
			\item k Nearest Neighbours: k wird festgelegt und Volumen wird so lange vergrößert bis k Samples gefunden wurden
			\item man kann zeigen dass wenn man $k_n$ langsamer wachsen lässt als n, die erzeugten Volumen $V_n$ gegen 0 konvergieren: $p_n(x) = \frac{k_n}{n*V_n} \approx p(x)$ mit $\lim\limits_{n\rightarrow \infty} p_n(x) = x$
		\end{itemize}
		\item $p_n(x) = \frac{k_n}{n*V_n} \approx p(x)$ in der Praxis 
		\begin{itemize}
			\item sei $n = 1$. Es gibt also genau 1 Sample $x_1$ und wir haben $k_n = \sqrt{n} = 1$
			\item gegeben sei ein Testpunkt x
			\item Das kleinste Volumen $k_1$, des kleinstenm Hyperwürfels mit Zentrum x, in der wir gerade noch $x_1$ finden? $2|x-x_1|$
			\item daraus ergibt sich eine sehr schlechte Schätzung: $p_1(x) = \frac{1}{2|x-x_1|}$
		\end{itemize}
		\item Verhalten für k-NN Dichteschätzung: für ein n kann die k-NN Dichteschätzung ziemlich stachelig sein
		\item Klassifikation
		\begin{itemize}
			\item n Samples $x_i$ aus c Klassen
			\item suchen k nächsten Samples in der Umgebung eines Punktes x
			\item $k_j$ Anzahl der Samples aus k welche zur Klasse $w_j$ gehören ($\sum_{j=1}^{c}k_j = k$)
			\item mit $\hat{p}(x,w_j) = \frac{k_j}{n/V}$ erhalten wir: $\hat{P}(w_j|x) = \frac{p(x,w_j)}{p(x)} = \frac{k_j}{k}$ alsop der Stimmanteil für $w_j$ relativ zur Gesamtanzahl der Stimmen $k$
			\item Klassifikation nach: $argmax_{w_j}\hat{P}(w_j|x)$
			\item Um einen Punklt x zu klassifizieren: Bestimme die k nächstliegenden Trainingspunkte $x_i$ und weise x diejenige Klasse zu, die unter diesen k Punkten am häufigsten Auftritt
		\end{itemize}
	\end{itemize}
\section{Support Vector Machines}
	\subsection{Einführung}
	\begin{itemize}
		\item Geg: Zwei Klassen in einem Merkmalsraum mit nichtlinearer Trennbarkeit
		\item Lösung: Transformation in einen höherdimensionalen Merkmalsraum
		\item Allg. die Transformation ist unbekannt welche eine lineare Trennbarkeit erreicht
		\item Schrotflinten-Verfahren: Wähle eine geeignete Menge von Basisfunktion und probiere rum :D
		\item Wie findet man einen geeigneten Satz von Transformation $\phi: R^d \rightarrow R^f$
		\item Wie findet man eine für die transformierte Punktemenge eine geeignete lineare Entscheidungsgrenzen im $R^f$?
		\item Wie sorgt man dafür dass die Klassifikationkomplexität...?
	\end{itemize}
	\subsection{Entscheidungsgrenzen}
	\begin{itemize}
		\item gegeben eine Transformation $\phi: R^d \rightarrow R^f$ und Trainingsdaten und Klassenzugehörigkeit
		\item gesucht lineares Modell: $y(x) = w^t\phi(x)+b$ so dass die Ebene $y(x) = 0$ die beiden Klassen trennt
		\item Gesucht wird als für gegebene $x_i$ eine möglichst gute Trenngrenze
		\item Wir suchen diejenige Ebene $y(x) = 0$ die den Abstand zu den jeweils am nächsten liegenden Punkten beider Klassen maximiert
		\item Diejenige Punkte die der Ebene $y(x) = 0$ am nächsten liegen stützen die Ebene ab $\rightarrow$ Stützvektoren
		\item Mathematisch: Gesucht $y(x) = w^t\phi(x)+b = 0$ die den minimalen Abstand der Punkte zu ihr maximiert also de Lösung für: $argmax_{w,b}(\frac{1}{||w||}min_i(t_i(w^t\phi(x_i)+b)))$
		\item Problem: schwieriges Optimierungsproblem
		\item Lsg: Umwandlung in ein einfacheres
		\item Beobachtung: Wenn wir von gegebenen w,b zu skalierten Werten k*w und k*b übergehen ändert sich das Ergebnis nicht d.h $\frac{t_i(w^t\phi(x_i)+b)}{||w||} = \frac{t_i((k*w)^t\phi(x_i)+k*b)}{||k*w||}$
		\item für gegebenes w,b finden wir also immer ein k so dass ür die skalierte Lösung $\frac{t_i((k*w)^t\phi(x_i)+k*b)}{||k*w||} = \frac{t_i(w'^t\phi(x_i)+b')}{||w'||} = \frac{1}{||w'||}$
		\item Ansatz: suche einfach k so dass gilt:$t_i(w'^t\phi(x_i)+b') = t_i((k*w)^t\phi(x_i)+k*b)=1$ gilt: wir benennen $w'$ und $b'$ um in w und b
		\item für die der optimalen Entscheidungsgrenze am nächsten liegen gilt: $t_i(w^t*\phi(x_i)+b) = 1$
		\item Für alle Punkte fordern wir also: $t_i(w^t*\phi(x_i)+b) \geq 1$ (kanonische Repräsentation der Entscheidungsgrenze)
		\item Punkte mit Wert = 1 sind aktive Punkte
		\item Vereinfachtes Optimierungsproblem: $argmax_{w,b}(\frac{1}{||w||}min_i(t_i(w^t\phi(x_i)+b))) = argmax_{w,b} \frac{1}{||w||}$ und die kanonische Repräsentation erfüllt
		\item Statt $\frac{1}{||w||}$ zu maximieren können wir auch $||w||^2$ minimieren d.h. die suchen $argmin_{w,b} \frac{1}{2}||w||^2$ + Nebenbedingungen
		\item Problem: quadratische Programmierung d.h. Konvexe Problemstellung besitzen ein globales Minimum
	\end{itemize}
	\subsection{Lagrange-Multiplikatoren}
	\begin{itemize}
		\item gegeben Funktion $f(x)$ die wir optimieren wollen und eine Bedingung $g(x) = 0$ die erfüllt werden muss
		\item $\nabla_x f(x) + \lambda\nabla_x g(x) = 0$. Man bezeichnet $\lambda$ als Lagrange-Multiplikator
		\item erweiterte Funktion: $L(x,\lambda) = f(x) + \lambda g(x)$
		\item Optimum muss gelten: $\nabla_x L(x,\lambda) = 0$ sowie $\nabla_\lambda L(x,\lambda) = g(x) = 0$
		\item Wie mit Ungleichungen?
		\begin{enumerate}
			\item Optimum liegt innerhalb des Constraintbereichs also $g(x_B) > 0$ und Gradient von g spielt keine Rolle also $\lambda = 0$
			\item Optimum liegt auf dem Constraint also $g(x_A) = 0$ und Gradient ist antiparallel zum Gradient von f d.h. $\nabla f(x) = -\lambda\nabla g(x)$ für $\lambda > 0$
		\end{enumerate}
	\end{itemize}
	\subsection{Bestimmung der Entscheidungsgrenze}
	\begin{itemize}
		\item Die Formulierung mit Lagrange-Multiplikatoren $a_i \geq 0$ für die Nebenbedingung ergibt sich zu: $L(w,b,a) = \frac{1}{2}||w||^2 - \sum_{i=1}^{n}a_i (t_i(w^t\phi(x_i)+b)-1)$
		\item Minuszeichen weil wir in Bezug auf w und b minimieren
		\item Ableitungen:
		\begin{itemize}
			\item $w = \sum_{i=1}^{n} a_i t_i \phi(x_i)$ (verbraucht den $1/2$ Faktor)
			\item $0 = \sum a_i*t_i$
		\end{itemize}
		\item Ableitung einsetzen um w und ?? zu eliminieren
		\item neuen Problem: $\hat{L}(a) = \sum a_i - \frac{1}{2}\sum_i\sum_j a_ia_jt_it_jk(x_i,x_j)$ mit $a_i \geq 0$ und $\sum_i a_it_i = 0$
		\item Die Kerne $k(x_i,x_j)$ sind hierbei definiert durch $k(x,x') = \phi(x)^t\phi(x')$
		\item Wenn wir das duale Optimierungsproblem gelöst haem erhalten für die $a_i$ über die sich dann auch b bestimmen lässt
		\item Klassifikation über: $y(x) = w^t\phi(x)+b$
		\item Wenn wir das gefundene $w$ einsetzen mit $w = \sum_{i=1}^{n}a_it_i\phi(x_i)$ ergibt sich: $y(x) =\sum_{i=1}^{n}a_it_i\phi(x_i)^t\phi(x_i) +b = \sum_{i=1}^{n}a_it_ik(x_i,x)+b$
		\item Optimierungsproblem ist definiert durch:
		\begin{itemize}
			\item Funktion: $\bar{L}(a) = \sum_{i=1}^{n}a_i-1/2\sum_{i=1}^{n}\sum_{j=1}^{n}a_ia_jt_it_jk(x_i,x_j)$
			\item Krush Kuhn-Tucker-Bedingungen:
			\item $a_i \geq0$
			\item $t_iy(x_i) -1 \geq 0$
			\item $a_i\{t_iy(x_i)-1\} = 0$
		\end{itemize}
		\item für den Punkt $x_i$ gilt also entweder $a_i = 0$ oder $t_iy(x_i) = 1$ d.h. entweder der Punkt spielt in der Klassifikation keine Rolle ODER er hat den minimalen Abstand zur Entscheidungsebene (Stützvektor)
		\item wenn wir einen Wert a haben dann können wir b ermitteln
		\item für einen Stützvektor $x_j$ gilt:
		\begin{itemize}
			\item $t_jy(x_j) = 1$
			\item also $t_j(\sum_{i \in S}^{n}a_it_ik(x_i,x_j)+b) = 1$
		\end{itemize}
		\item S die Menge der Indizes der Stützvektoren ($a_i$ der anderen Vektoren sind ja ohehin 0)
		\item numerisch stabilere Lösung: Durchschnitt der b Werte über alle Stützvektoren
	\end{itemize}
	\subsection{Kerne und Dimensionen}
	\begin{itemize}
		\item Was bedeutet $\hat{L}(a) = \sum_{i=1}^{n}a_i-1/2\sum_{i=1}^{n}\sum_{j=1}^{n}a_ia_jt_it_jk(x_i,x_j)$?
		\item Ein Kern misst die Ähnlichkeit zweier Vektoren
		\item um $\hat{L}$ zu maximieren, müssen wir ähnliche $x_i,x_j$ finden die unterschiedliche Klassen angehören $t_it_j = -1$ und diese mit großen $a_i,a_j$ wichten
		\item Diese erhöhen den Wert von $\hat{L}$ da die Summe mit negativen Vorzeichen in die Maximierung eingeht
		\item $w = \sum_{i=1}^{n}a_it_i\phi(x_i)$ bedeutet dann
		\item die Summe kann in zwei Teile zerlegt werden: die Summe der Klassen $t_{i} = 1$ und die Summe der Klasse $t_{i} = -1$: $w = \sum_{i\in t_{i} = 1} a_i\phi(x_{i})-\sum_{j \in t_{i} = -1} a_{j}\phi(x_{j})$
		\item w ist die Differenz des gewichteten Durchschnitts beider Klassen, bestimmt für die interessanten Vektoren
		\item Quadratische Optimierung mit M Variablen haben eine Komplexität von $O(M^3)$
		\item in der dualen Formulierung haben wir n Variablen die $a_i$. Also haben wir so viele Variablen wie Datenpunkte
		\item ursprünglich hatten wir f Variablen (die Komponenten von w), so viele wie Dimensionen in $R^f$
		\item wenn wir eine feste kleine Zahl f von Basisfunktionen haben bringt der Übergang in die duale Formulierung keinen großen Gewinn
		\item erhebliche Vorteile wenn die Merkmalsräume betrachten deren Dimensionalität die Anzahl der Datenpunkte übersteigt (Extrem: unendlich viel Dimensionen)
		\item Wie können wir mit unendlich vielen Dimensionen rechnen?
		\item $\hat{L}(a) = \sum_{i=1}^{n} a_i - 1/2 \sum_{i=1}^{n}\sum_{j=1}^{n}a_ia_jt_it_jk(x_i,x_j)$ und $y(x) = \sum_{i=1}^{n} a_it_ik(x_i,x)+b$ ... wir können $R^f$ als Skalarproddukt zweier transformierter Vektoren repräsentieren durch den Kern: $k(x,x') = \phi(x)^t*\phi(x')$
		\item d.h. wir können das Skalarprodukt berechnen ohne das Bild der Transformation $\phi(x)$ in $R^f$ selbst bestimmen zu müssen
		\item Beispiele:
		\begin{itemize}
			\item Polynome mit Grad f: $k(x,x') = (1+x^tx')^f$
			\item Radiale Basisfunktion (gaußsche Basis): $k(x,x') = exp(- ||x-x'||^2/2\sigma^2))$
			\item Logistischer Sigmoid: $k(x,x') = tanh(k_1*x^tx'+k_2)$
		\end{itemize}
		\item Die Skalarprodukte $x^tx'$ und Abstände $||x-x'||$ werden jeweils im ursprünglichen d-dimensionalen Merkmalsraum $R^d$ berechnet
	\end{itemize}
	\subsection{Überlappende Entscheidungsgrenzen}
	\begin{itemize}
		\item Bisher: Annahme dass Punkte perfekt linear separierbar in $R^f$ sind
		\item Jetzt: Führe zusätzliche Schlupfvariablen $v_i \geq 0$ die das Eindringen von $x_i$ in die verbotene Zone beschreiben
		\item Ziel: Minimierung einer modifizierten Zielfunktion: $C \sum_{i=1}^{n}v_i +1/2*||w||^2$; $C > 0$ steuert den Kompromiss zwischen der Strafe durch die Schlupfvariablen und der Komplexität der Entscheidungsfläche
		\item $v_i = 0$ für alle $x_i$ ausserhalb des Randbereiches bzw. auf dem Rand
		\item $v_i < 1$ für Punkte hinnerhalb des Randbereiches aber auf der richtigen Seite
		\item $v_i > 1$ falls Punkt auf der falschen Seite
		\item falls $v_i > 0$ gilt also $v_i = |t_i - y(x_i)$
		\item Anpassung der Bedingung $t_iy(x_i) \geq 1$ durch Bedingung: $t_iy(x_i) \geq 1 -v_i$
		\item Die Summe der $v_i$ ist eine Strafe für die Verletzung des Randbereiches. je größer ein $v_i$ desto größer die Verletzung
		\item in der Zielfunktion muss jetzt nicht nur $||w||^2$ minimiert werden sondern auch die Strafe
		\item Ziel: min $C\sum_{i=1}^{n}v_i + 1/2 ||w||^2$
		\item da für jeden falschen klassifizierten Punkt gilt $v_i > 1$ stellt $\sum_{i=1}^{n}v_i$ eine Obergrenze für die Anzahl der falsch klassifizierten Punkte da
		\item C ermöglicht es den Kompromiss zwischen Verletzung des Randbreichs un Komplexitäöt der Entscheidungsgrentze zu wählen
		\item je größer C desto größer wird die Bedeutung $v_i$ desto größer wird tendenziell $||w||$ um $v_i$ klein zu halten
		\item je größer $||w||$ desto enger umkurvt die Entscheidungsgrenze die Stützvektoren
		\item bester Wert für C wird sinnvollerweise wieder durch Kreuzvalidierung bestimmt
		\item Parameter für SVM:
		\begin{itemize}
			\item C
			\item Form des Kerns
			\item Parameter des Kerns
		\end{itemize}
		\item Überanpassung erkennt man an einem schmalen Randbereich (großes w) und einer großen Anzahl der Stützvektoren
		\item groß ist problemabhängig
	\end{itemize}
	\section{Nichtmetrische Methoden: Bäume}
	\subsection{Einführung}
	\begin{itemize}
		\item Bisher: Klassifikationsferfahren brauchten eine Metrik
		\item Problem: Nominalskalen haben keine Metrik (Abstandsbegriff)
		\item Häufig liegen Daten zu Objekten als Eigenschaftslisten vor; Eigenschafte haben keinen Abstandsbegriff
		\item Lösung: Klassifikation durch Entscheidungsbaum
		\item Abstieg von Wurzel in die Blätter; jeder Knoten überprüft ein Merkmal
		\item Vorteil: sehr leicht interpretierbar d.h. Grund für Klassifikation direkt klar da die Merkmalsausprägungen im Muster die Konjunktion der Eigenschaftsbelegungen entlang des Pfades zu Klassifikation erfüllt
	\end{itemize}
	\subsection{Aufbau von Bäumen}
	\begin{itemize}
		\item Betrachte $A \subset D$ der Testdaten
		\item Wenn alle Instanzen in A derselben Klasse $w_A$ angehören ist die Teilmenge rein, dann kann ein Blattknoten angelegt werden der der Klasse $w_A$ zugeordnet wird
		\item Wenn die Instanzen in A unterschiedlichen Klassen angehören muss eine Entscheidungs getroffen werden
		\begin{itemize}
			\item Trotzdem Blattknoten anlegen und Klassifikation gemäß der Mehrheit
			\item inneren Knoten anlegen und A in mehere Teilmengen $A_j$ aufspalten; Rekursives Verfahren auf $A_j$
		\end{itemize}
		\item Anzahl der Verzweigung
		\begin{itemize}
			\item Sollte man eine Eigenschaft mit n Ausprägungen mit meiner n-Wege Verzweigung teilen?
			\item im allgemeinen werden dadurch die Daten aber zu stark fragmentiert wodurch eine sinnvolle Zerlegung behindert wird
			\item eine n-Wege-Zerlegung kann durch mehere binäre Zerlegungen repräsentiert werden deshalb wird dieser Ansatz oft bevorzugt
		\end{itemize}
		\item Auswahl der Eigenschaft
		\begin{itemize}
			\item Eigenschaft für die Zerlegung ob diese eine Kombination aus meheren Merkmalen (polytetisch) oder nur ein Merkmal(monothetisch) beinhalte
			\item monothetische Eigenschaften zerlegen den Merkmalsraum in Regionen deren Grenzen Hypereben sind die Orthogonal zu den jeweils gewählten Merkmalsdimensionen liegen
		\end{itemize}
	\end{itemize}
	\subsection{Reinheit von Knotens}
	\begin{itemize}
		\item Grundlegendes Prinzip der Merkmalsauswahl ist möglichst einfache Bäume zu erzeugen (flach und wenige Knotens) (vergleiche Ockhams Rasiermesser)
		\item um dies zu erreichen wählen wir in jedem Knoten N diejenige Eigenschaft T, welche die Reinheit der Kinderknoten maximiert
		\item Unreinheit eines Knoten definiert durch die Durchmischtheit der dem Knoten zu geordneten Daten in Bezug auf ihre Klassenzugehörigkeit $\rightarrow$ einfacher definiert als die Reinheits
		\item die Unreinheit eines Knoten N bezeichnen wir als $i(N)$
		\item für Reinheitsmaße $i(N)$ soll gelten:
		\begin{itemize}
			\item $i(N) = 0$ falls deer Knoten rein ist
			\item $i(N)$ um so größer je stärker der Knoten durchmischt
		\end{itemize}
		\item ein Ansatz für Unreinheit: Informationen die wir brauchen um zu einer Instanz $x \in N$ die zugehörige Klasse zu bestimmen
		\item Wie viel Information brauchen wir falls N ein gemischter Knoten ist in dem die einzelnen Klassen i mit relativen Häufigkeiten $p_i$ enthalten sind und man uns sagt dass $x \in N$ zu Klasse $j$ gehört? $log_2 \frac{1}{p_j} = -log_2(p_j)$ Bits
		\item Insgesamt ergibt damit der Erwartungswert der Information (Informationsentropie): $i(N) = -\sum_{i=1}^{c}p_i log_2 p_i$
		\item Basis für die Auswahl einer Zerlegung ist dann die Suche nach einer Eigenschaft T, die zwei Kindknoten $N_L,N_R$ erzeugt die die Unreinheit soweit wie möglich minimiert
		\item für Informationsentropie: eine Zerlegung die so wenig wie möglich zusätzliche Information erforderlich macht um die Klasse einer Instanz zu bestimmen die im Teilbaum von N liegt (ein Test T der möglichst viel Information liefert)
		\item Annahme: haben Zerlegung bei der ein Anteil von $P_L$ Instanzen im linken Kindknoten $N_L$ landet und ein Anteil von $(1-P_L)$ Instanzen im rechten Kindknoten $N_R$
		\item Renheitsgewinn: $\delta i(N) = i(N)-P_Li(N_L)-(1-P_L)i(N_R)$
		\item Maximierung des Reinheitsgewinn durch die sinnvolle Wahl der Eigenschaft T mit deren Hilfe die Zerlegung durchgeführt wird (Wenn Informationsentropie als Maß der Unreinheit repräsentiert $\delta i(N)$ den Gewinn an Informationen in Bits
		\item weitere Reinheitsmaße:
		\begin{itemize}
			\item $i_{gini}(N) = \sum_{i=1}^{c}p_i(1-p_i)$: Gini-Index ist der erwartete Fehler wenn man einer zufälligen Instanz aus N ein zufälligesLabel aus N zuweist. Gini-Index als Maß der Varianz der Zufallsstichprobe in Ns
			\item $i_{bayes}(N) = 1 - max_i p_i$: Ist der Bayes-Fehler für die Klassifikation einer Instanz auf BAsis der Stichprobe iN
		\end{itemize}
		\item unterschiedliche Reinheitsmaße für die Optimierung unterschiedlicher Kriterien
		\begin{itemize}
			\item Informationentropie: Mittlere Information in Bits die erforderlich ist um eine Instanz im Knoten zu klassifizieren
			\item Gini-Index: Varianz der Stichprobe in n (bzgl der Verteilungsfunktion der Klassenlabel)
			\item Bayes: Fehler bei Klassifikation gemäß der Bayes-Regels
		\end{itemize}
		\item Differenzierbarkeit von Gini und Entropie hilfreich wenn das Maximum über einen kontinuierlichen Parameterrraum bestimmt werden muss
		\item Gini und Entropie reagieren sensitiver auf Änderungen der Klassenverteilungen in den Knoten in Folge eines Splits
	\end{itemize}
	\subsection{Eigenschaften und Attribute}
	\begin{itemize}
		\item Split einer Instanzmenge A wird durch eine Eigenscahft T(x) definiert aus der sich dann die Teilmengen ergeben
		\item bevorzugt binäre Splits mit boolschen Funktion T(x)
		\item für die Teilmenge des Splits: $A_L = \{x\in A|T(x)\},A_R = \{x\in A|\neg T(x)\}$
		\item binäre Splits: Vorteil eindimensionales Optimierungsproblem, n-äre Splits müssen höherdimensionale Optimierungsprobleme lösens
		\item wie kann T sinnvoll definiert werden $\rightarrow$ hängt von den Skalenniveau ab
		\item metrische Attribute: $x_i$ sucht man typischerweise eine Konstante $s$ so dass ein Test die Form $x_i < s$ annimmt
		\item Idee: optimierende Funktion $\delta i(N)$ als Funktion von s dar und kann dann numerische Optimierungsverfahren verwenden um s zu bestimmen das den maximalen Gewinn liefert
		\item Idee geht auch für Kombination von realwertigen Attributen $\rightarrow$ Split beschreibt eine Hypereben im Teilraum
		\item man kann auch die nach Merkmal sortierte Liste der Instanzen betrachten um Schnittpunkte s der Form $x_l < s < x_u$ zu bestimmen $\rightarrow$ wahl durch den Mittelwert, gewichteten Durchschnitt $(1-P)x_l+Px_u$ wobei P die Wahrscheinlichkeit dafür angibt dass ein Muster im linken Bereich liegt
		\item ordinale Attribute: Durchmusterung einer ach Attributwert sortierten Liste von Instanzen anwendbar
		\item nominale Attribute: jede Zerlegung der Menge der Attributwerte in zwei Teilmengen als Schnittpunkt betrachtet $\rightarrow$ b Attributewerte sind $2^{b-1}$ mgl Zerlegung (erheblicher Aufwand)
		\item man könnte ordinale Attribute direkt für die Zerlegung von A in b Teilmegen (b-Wege-Splitss) nutzen: Reinheitsgewinn: $\Delta i(N) = i(N) - \sum_{k=1}^{b} P_k i(N_k)$s mit $P_k$ der Anteil der Instanzen ist, bei denen das Attribut den k-ten Attributwert
		\item Nachteil: Splits mit großem b bevorzugt auch wenn sie keine sinnvolle Struktur in den Daten repräsentieren (2 Klassen aber 3 Wege
		\item dehalb muss der Reinheitsgewinn für den Split relativ zur Menge an Informationen betrachtet werden die schon on der Struktur der Aufteilung selbst enthalten $\Delta_b i(N) = \frac{\Delta i(N)}{-\sum_{k=1}^{b} P_k log_2 P_k}$
		\item Split sind nur lokale Operationen und erzeugen nur ein lokales Optimum ohne sich für die globalen Auswirkungen zu berücksichtigen $\rightarrow$ Baum entspricht nicht dem globalen Optimum
	\end{itemize}
	\subsection{Ende des Wachstum}
	\begin{itemize}
		\item Wie lang wächst ein Baum?
		\item Extrem: so lange bis jedes Blatt nur eine Instanz enthälts
		\item Nachteile:
		\begin{itemize}
			\item Baum sehr groß
			\item Baum hat Trainingsdaten auswendig gelernt $\rightarrow$ je größer der Baum desto eher enthält Tests die sich auf verrauschte Merkmale beziehen
		\end{itemize}
		\item Ziel: Performance des Baumes überprüfen mit einem Testdatensatz
		\item Validierungsverfahren / Lösungens
		\begin{itemize}
			\item Aufspaltung der Daten für Training und zum Testen, solange Baum neue Ebene generieren bis die Validierung ein Wiederabsinken der Klassifikationsgenauigkeit anzeigt
			\item Schwellwert $\beta$ für den Reinheitsgewinn festlegen $\rightarrow$ Knoten wird nur gesplittet wernn der Reinheitsgewinn größer als dieser Schwellwert $\Delta i(N) > \beta$ (Vorteil: alle Daten stehen für das Training zur Verfügung)
			\item Festlegung dass jedes Blatt mindestens n Instanzen oder einen relativen Anteil von p aller Trainingsdaten enthalten muss
			\item globales Abbruchkritierum: $\alpha*size + \sum_{leaf nodes} i(N)$, size ein Maß für die Größe des Baumes $\rightarrow$ Baum wächst solange bis die gewichtete Summe aus Komplexität des Baumes und restlicher Unreinheit eine bestimmte Größe erreicht hats, Kompromiss zwischen der Komplexität des Baums und dem restlichen Klassifikationsfehler möglich (Problem: $\alpha$ sinnvoll festzulegen)
			\item Signifikanztets verwenden um zu prüfen ob ein Split eine statistisch signifikate Verbesserung der Reinheit erzeugt (Chi-Square-Test); Problem: Wachstum kann zu früh beendet werden da nicht vorrausschauend; Lösung: Pruning
		\end{itemize}
	\end{itemize}
	\subsection{Zurückschneiden}
	\begin{itemize}
		\item Horizont-Effekt vermeiden dem umgekehrten Weg wählen: Baum bis maximale Größe wachsen lassen und danach den Baum durch Vereinen benachbarte Knoten auf eine sinnvolle Größe zurückschneiden lassen
		\item Reduced Error Pruning
		\begin{itemize}
			\item Zerlegung der Daten in Trainingsdaten und Validierungsdaten
			\item Erzeuge maximalen Baum für Traningsdaten
			\item für jeden Knoten Prüfe Performance für Validierungsdaten unter Bedingung das der Teilbaum an diesem Knoten gelöscht wird
			\item Lösche denjenigen Knoten+Teilbaum für den die Performance maximiert wird
			\item Ersetze den Knoten durch ein Blatt mit der Mehrheitsklassifikation des gelöschten Teilbaum
			\item Ende wenn Präzision für Validierungsdaten sinkt
		\end{itemize}
		\item Cost Complexity Pruning
		\begin{itemize}
			\item Maß für Gesamtkomplexität des Baums als gewichtete Summe von Größe und Restunreinheit,
			\item $|T|$ die Anzahl der Blätter eines Baumes T
			\item Gesamtkomplexität: $C_\alpha(T) = \alpha|T| + \sum_{m=1}^{|T|}N_m i(m)$ mit $N_m$ die Anzahl der Instanzen im Blatt m ist
			\item aus Maximalen Baum wird eine Sequenz immer kleinerer Bäume erzeugt in dem jeweils derjenige innere Knoten kollabiert wird der das Anwachsen von: $\sum_{m=1}^{|T|}N_m i(m)$ minimiert
			\item finden von $\alpha$ erfolgt mit Hilfe eines Validierungsansatzen, wähle $\hat{\alpha}$ mit dem das beste Ergebnis für einen separaten Testdatensatz erreicht wird
		\end{itemize}    
		\item Regelansatz (C4.5)
		\begin{itemize}
			\item Pfad von Wurzel zu Blatt als Regel darstellen: Konjunktion aller Eigenscahften
			\item Regeln können unabhängig voneinander durch Löschen von Vorbedingungen beschnitten erden so dass die Präzision verbessert
			\item resultierende Regelmenge wird dann in der Reihenfolge der geschätzten Präzision sortiert
			\item für unterschiedliche Pfade kann unterschiedlich beschnitten werden ohne eine Reorganisation des Baumes erforderlich ist
		\end{itemize}
	\end{itemize}
	\subsection{Klassifikation}
	\begin{itemize}
		\item reine Blätter erhalten das homogene Klassenlabel der ihrer Instanzen
		\item unreine Blätter erhalten das Klassenlabel der Mehrheit
	\end{itemize}
	\subsection{Wahl der Attribute}
	\begin{itemize}
		\item Es ist hilfreich wenn die Attribute gut gewählt werden, so dass die Tests im Baum auch senkrecht zu den effektiven Attributdimensionen sind (Vorverarbeitung durch PCA)
	\end{itemize}
	\subsection{Fehlende Attribute}
	\begin{itemize}
		\item Situation: Fragenbögen sind unvollständig
		\item Lösung 1: Weise den häufigsten Wert alle Instanzen im aktuellen Knoten zu
		\item Lösung 2: erzeuge virtuelle Punkte, für jeden möglichen attributwert einen, jeder Pu nkt erhält ein Gewicht entsprechend der Häufigkeit des Wertes, Führe für alle virtuellen Punkte die weitere Klassifikation durch; bestimme die endgültige Klasse durch Abstimmung unter den q Punkten
		\item Während der Konsturktion des Baumes: so tun als wäre der Punkt nicht vorhanden
	\end{itemize}
	\subsection{Bewertung}
	\begin{itemize}
		\item Umgang mit unterschiedlichen Skalenniveaus
		\item Fehlerbehaftete Daten, fehlende Daten, viele Attribute
		\item nichtlineare Entscheidungsgrenzens
	\end{itemize}
\bibliography{library}
\bibliographystyle{plain}

\end{document}